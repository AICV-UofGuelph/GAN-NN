{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img+1, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0)\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size) # embeds num of classes (num paths) to vectors of img_sz^2\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True), \n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim = 1) # N, channels, img height, img width\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_noise,\n",
    "        channels_img,\n",
    "        features_g,\n",
    "        num_classes,\n",
    "        img_size,\n",
    "        embed_size\n",
    "        ):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size) #embedding needs to be added to noise\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # latent vecotr z: N x noise_dim x 1 x 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim = 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, labels, real, fake, device=\"cuda\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    if real.shape != fake.shape:\n",
    "        print(\"not same shape\")\n",
    "    interpolated_images = real * epsilon + fake * (1-epsilon) #interpolape epsilon % real image, (1-epsilon fake image)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # compute grad of mixed scores w.r.t interpolated image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to override __init__, __len__, __getitem__\n",
    "# # as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, bad_path, transform=None, label_transform=None, shape = (100,100)):\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        self.path_labels = []\n",
    "        \n",
    "        # load  the \"good\" paths (Class 1)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "                self.path_labels.append(1)\n",
    "\n",
    "        # load  the \"bad\" paths (Class 0)\n",
    "        for filename in os.listdir(bad_path):\n",
    "            with open(os.path.join(bad_path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "                self.path_labels.append(0)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "        label = torch.tensor(self.path_labels[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x).cuda()\n",
    "\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label).cuda()\n",
    "\n",
    "        return x, label\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 200\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "GEN_EMBEDDING = 100\n",
    "\n",
    "#Speicific to WGAN\n",
    "CRITIC_ITERATIONS = 5 # how many times the critic loop runs for each generator loop\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "# if we load from file this will be set to the loaded epoch\n",
    "epoch_loaded = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToPILImage(),\n",
    "        # tfms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "\n",
    "dataset = PathsDataset(path = \"./data/map_64x64/\", bad_path=\"./data/map_64x64_bad/\", shape = (64,64), transform=transforms, label_transform=label_transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print a training path from dataset\n",
    "idx = 20000\n",
    "image = dataset[idx][0][0].cpu()\n",
    "plt.imshow(image)\n",
    "print(f'class: {dataset[idx][1]}')\n",
    "print(dataset[idx][0][0].device)\n",
    "print(dataset[idx][1].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (embed): Embedding(2, 4096)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/200                   Loss D: -10.9004, loss G: 11.3923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5183/1915066986.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayesianNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayesianNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayesianNN/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayesianNN/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5183/4216303701.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_path = \"./checkpoints/conditional/generator/\"\n",
    "disc_path = \"./checkpoints/conditional/discriminator/\"\n",
    "loaded = 0 # if we haven't loaded from file\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    if epoch_loaded !=0 and loaded == 0:\n",
    "        print(f'loaded from file. On epoch: {epoch_loaded}')\n",
    "        loaded = 1\n",
    "        epoch = epoch_loaded\n",
    "    for batch_idx, (real, labels) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise, labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise, labels)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, labels, real, fake, device=device) # compute the gradient penalty\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP*gp\n",
    "            ) #   want to maximize (according to paper) but \n",
    "                                                                            #   optim algorithms are for minimizing so take - \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # want to re use the computations for fake for generator\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise, labels)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            # save generator checkpoint\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': gen.state_dict(),\n",
    "                        'optimizer_state_dict': opt_gen.state_dict(),\n",
    "                        'loss': loss_gen,\n",
    "            }, gen_path + \"epoch-\" + str(epoch) + \"_batch-\" + str(batch_idx) + \".tar\")\n",
    "\n",
    "            # save discriminator checkpoint\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': critic.state_dict(),\n",
    "                        'optimizer_state_dict': opt_critic.state_dict(),\n",
    "                        'loss': loss_critic,\n",
    "            }, disc_path + \"epoch-\" + str(epoch) + \"_batch-\" + str(batch_idx) + \".tar\")\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9896, -0.9998, -0.9998,  ..., -1.0000, -1.0000, -0.9999],\n",
      "          [-0.9964, -0.9996, -0.9999,  ..., -0.9997, -1.0000, -0.9999],\n",
      "          [-0.9990, -0.9999, -0.9997,  ..., -1.0000, -1.0000, -0.9998],\n",
      "          ...,\n",
      "          [-0.9999, -0.9997, -1.0000,  ..., -1.0000, -1.0000, -0.9999],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9990, -1.0000, -0.9995,  ..., -0.9999, -0.9995, -0.9939]]]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gen real and fake\n",
    "label = 1\n",
    "noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "plt.imshow(fake.cpu().detach().numpy()[0][0])\n",
    "print(fake)\n",
    "print(fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-46.6801], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = critic(fake, torch.tensor([1], dtype=torch.int32, device=device)).reshape(-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0\n",
      "torch.Size([1, 1, 64, 64])\n",
      "tensor([-43.3079], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print a training path from dataset and test discrim on it\n",
    "idx = 20000 #index of trainming dataset to test on (0-1999 good, 20000-39999 bad (0))\n",
    "image = dataset[idx][0][0]\n",
    "plt.imshow(image.cpu())\n",
    "print(f'class: {dataset[idx][1]}')\n",
    "image = image[None, None, :] #add dmy dimensionms to put through descriminator\n",
    "print(image.shape)\n",
    "\n",
    "#test discrim\n",
    "output = critic(image, torch.tensor([0], dtype=torch.int32, device=device)).reshape(-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen 10 good\n",
    "for idx in range(10):\n",
    "    label = 1 # good path = 1\n",
    "    noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "    # filename = \"path_\"+str(idx)+\"_.txt\"\n",
    "    # np.savetxt(filename, image, delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.imshow(fake.cpu().detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen 10 bad\n",
    "for idx in range(10):\n",
    "    label = 0 # bad path = 0\n",
    "    noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "    # filename = \"path_\"+str(idx)+\"_.txt\"\n",
    "    # np.savetxt(filename, image, delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.imshow(fake.cpu().detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "last_epoch = \"epoch-14_batch-0.tar\"\n",
    "#load gen from file\n",
    "gen_path = \"./checkpoints/conditional/generator/\"\n",
    "\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "\n",
    "checkpoint_gen = torch.load(gen_path+last_epoch)\n",
    "gen.load_state_dict(checkpoint_gen['model_state_dict'])\n",
    "opt_gen.load_state_dict(checkpoint_gen['optimizer_state_dict'])\n",
    "epoch_loaded = checkpoint_gen['epoch']\n",
    "loss = checkpoint_gen['loss']\n",
    "\n",
    "\n",
    "#load critic from fle\n",
    "disc_path = \"./checkpoints/conditional/discriminator/\"\n",
    "\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "\n",
    "checkpoint_critic = torch.load(disc_path+last_epoch)\n",
    "critic.load_state_dict(checkpoint_critic['model_state_dict'])\n",
    "opt_critic.load_state_dict(checkpoint_critic['optimizer_state_dict'])\n",
    "epoch = checkpoint_critic['epoch']\n",
    "loss = checkpoint_critic['loss']\n",
    "\n",
    "\n",
    "# train or eval()\n",
    "# gen.eval()\n",
    "# critic.eval()\n",
    "# - or -\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "print(epoch_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bayesianNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c0cf972fe55eaf0c962c4929f592d86a72c532b00283f932a90435beee88e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
