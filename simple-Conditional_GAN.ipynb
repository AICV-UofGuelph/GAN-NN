{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAP_NAME = \"map_64x64\"\n",
    "MAP_DIMS = (64,64)\n",
    "\n",
    "FEATURES_GEN = 64\n",
    "FEATURES_DISC = 64\n",
    "NOISE_DIM = 100\n",
    "IMG_CHANNELS = 1\n",
    "IMAGE_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 200\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "GEN_EMBEDDING = 100\n",
    "\n",
    "#Speicific to WGAN\n",
    "CRITIC_ITERATIONS = 5 # how many times the critic loop runs for each generator loop\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "# if we load from file this will be set to the loaded epoch\n",
    "epoch_loaded = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img+1, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0)\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size) # embeds num of classes (num paths) to vectors of img_sz^2\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True), \n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim = 1) # N, channels, img height, img width\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_noise,\n",
    "        channels_img,\n",
    "        features_g,\n",
    "        num_classes,\n",
    "        img_size,\n",
    "        embed_size\n",
    "        ):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size) #embedding needs to be added to noise\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # latent vecotr z: N x noise_dim x 1 x 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim = 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, labels, real, fake, device=\"cuda\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    if real.shape != fake.shape:\n",
    "        print(\"not same shape\")\n",
    "    interpolated_images = real * epsilon + fake * (1-epsilon) #interpolape epsilon % real image, (1-epsilon fake image)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # compute grad of mixed scores w.r.t interpolated image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to override __init__, __len__, __getitem__\n",
    "# # as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, bad_path, transform=None, label_transform=None, shape = (100,100)):\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        self.path_labels = []\n",
    "        \n",
    "        # load  the \"good\" paths (Class 1)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "                self.path_labels.append(1)\n",
    "\n",
    "        # load  the \"bad\" paths (Class 0)\n",
    "        for filename in os.listdir(bad_path):\n",
    "            with open(os.path.join(bad_path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "                self.path_labels.append(0)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "        label = torch.tensor(self.path_labels[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x).cuda()\n",
    "\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label).cuda()\n",
    "        else:\n",
    "            label = label.cuda()\n",
    "\n",
    "        return x, label\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToPILImage(),\n",
    "        # tfms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(IMG_CHANNELS)], [0.5 for _ in range(IMG_CHANNELS)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PathsDataset(path = f\"./env/{MAP_NAME}/paths/good_paths/\", bad_path=f\"./env/{MAP_NAME}/paths/bad_paths/\", shape = MAP_DIMS, transform=transforms, label_transform=label_transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen = Generator(NOISE_DIM, IMG_CHANNELS, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "critic = Discriminator(IMG_CHANNELS, FEATURES_DISC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a training path from dataset\n",
    "idx = 20000\n",
    "image = dataset[idx][0][0].cpu()\n",
    "plt.imshow(image)\n",
    "print(f'class: {dataset[idx][1]}')\n",
    "print(dataset[idx][0][0].device)\n",
    "print(dataset[idx][1].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"./checkpoints/conditional/generator/\"\n",
    "disc_path = \"./checkpoints/conditional/discriminator/\"\n",
    "loaded = 0 # if we haven't loaded from file\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    if epoch_loaded !=0 and loaded == 0:\n",
    "        print(f'loaded from file. On epoch: {epoch_loaded}')\n",
    "        loaded = 1\n",
    "        epoch = epoch_loaded\n",
    "    for batch_idx, (real, labels) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise, labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise, labels)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, labels, real, fake, device=device) # compute the gradient penalty\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP*gp\n",
    "            ) #   want to maximize (according to paper) but \n",
    "                                                                            #   optim algorithms are for minimizing so take - \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # want to re use the computations for fake for generator\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise, labels)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            # save generator checkpoint\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': gen.state_dict(),\n",
    "                        'optimizer_state_dict': opt_gen.state_dict(),\n",
    "                        'loss': loss_gen,\n",
    "            }, gen_path + \"epoch-\" + str(epoch) + \"_batch-\" + str(batch_idx) + \".tar\")\n",
    "\n",
    "            # save discriminator checkpoint\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': critic.state_dict(),\n",
    "                        'optimizer_state_dict': opt_critic.state_dict(),\n",
    "                        'loss': loss_critic,\n",
    "            }, disc_path + \"epoch-\" + str(epoch) + \"_batch-\" + str(batch_idx) + \".tar\")\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen real and fake\n",
    "label = 1\n",
    "noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "plt.imshow(fake.cpu().detach().numpy()[0][0])\n",
    "print(fake)\n",
    "print(fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = critic(fake, torch.tensor([1], dtype=torch.int32, device=device)).reshape(-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a training path from dataset and test discrim on it\n",
    "idx = 20000 #index of trainming dataset to test on (0-1999 good, 20000-39999 bad (0))\n",
    "image = dataset[idx][0][0]\n",
    "plt.imshow(image.cpu())\n",
    "print(f'class: {dataset[idx][1]}')\n",
    "image = image[None, None, :] #add dmy dimensionms to put through descriminator\n",
    "print(image.shape)\n",
    "\n",
    "#test discrim\n",
    "output = critic(image, torch.tensor([0], dtype=torch.int32, device=device)).reshape(-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen 10 good\n",
    "for idx in range(10):\n",
    "    label = 1 # good path = 1\n",
    "    noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "    # filename = \"path_\"+str(idx)+\"_.txt\"\n",
    "    # np.savetxt(filename, image, delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.imshow(fake.cpu().detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen 10 bad\n",
    "for idx in range(10):\n",
    "    label = 0 # bad path = 0\n",
    "    noise = torch.randn(1, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise, torch.tensor([label], dtype=torch.int32, device=device))\n",
    "    # filename = \"path_\"+str(idx)+\"_.txt\"\n",
    "    # np.savetxt(filename, image, delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.imshow(fake.cpu().detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = \"epoch-14_batch-0.tar\"\n",
    "#load gen from file\n",
    "gen_path = \"./checkpoints/conditional/generator/\"\n",
    "\n",
    "gen = Generator(NOISE_DIM, IMG_CHANNELS, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "\n",
    "checkpoint_gen = torch.load(gen_path+last_epoch)\n",
    "gen.load_state_dict(checkpoint_gen['model_state_dict'])\n",
    "opt_gen.load_state_dict(checkpoint_gen['optimizer_state_dict'])\n",
    "epoch_loaded = checkpoint_gen['epoch']\n",
    "loss = checkpoint_gen['loss']\n",
    "\n",
    "\n",
    "#load critic from fle\n",
    "disc_path = \"./checkpoints/conditional/discriminator/\"\n",
    "\n",
    "critic = Discriminator(IMG_CHANNELS, FEATURES_DISC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "\n",
    "checkpoint_critic = torch.load(disc_path+last_epoch)\n",
    "critic.load_state_dict(checkpoint_critic['model_state_dict'])\n",
    "opt_critic.load_state_dict(checkpoint_critic['optimizer_state_dict'])\n",
    "epoch = checkpoint_critic['epoch']\n",
    "loss = checkpoint_critic['loss']\n",
    "\n",
    "\n",
    "# train or eval()\n",
    "# gen.eval()\n",
    "# critic.eval()\n",
    "# - or -\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "print(epoch_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
