{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, transform=None, shape = (100,100)):\n",
    "        print(\"Loading paths dataset...\")\n",
    "        # Read in path files\n",
    "        # Convert to x by y np arrays\n",
    "        # add the np arrays to a list\n",
    "        # set self.transform and self.data\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "                \n",
    "                # xvales which to interpolate on\n",
    "                # want to interpolate on xvalues from the min xval in the path to the largest xval in the path\n",
    "                self.xvals = np.linspace(int(min(self.path[:,0])), int(max(self.path[:,0])), int(max(self.path[:,0])-min(self.path[:,0])))\n",
    "                self.xvals = self.xvals.astype(int)\n",
    "\n",
    "                # interpolate for all xvals using the paths from file's x and y values\n",
    "                self.interp_path = np.interp(self.xvals, self.path[:,0], self.path[:,1])\n",
    "                self.interp_path = np.array(self.interp_path).astype(int)\n",
    "\n",
    "                # create a LxW matrix where all the values where path is equal to 1\n",
    "                self.path_matrix = np.zeros(shape)\n",
    "                self.path_matrix[self.interp_path, self.xvals] = 1\n",
    "                \n",
    "\n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"getitem\")\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "        # imagePath = self.paths_file + \"/\" + self.data['Image_path'][idx]\n",
    "        # image = sk.imread(imagePath)\n",
    "        # label = self.data['Condition'][idx]\n",
    "        # image = Image.fromarray(image)\n",
    "\n",
    "        # if self.sourceTransform:\n",
    "        #     image = self.sourceTransform(image)\n",
    "        x = np.float32(self.paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE_DISC = 1e-7  # could also use two lrs, one for gen and one for disc\n",
    "LEARNING_RATE_GEN = 1e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 128\n",
    "NUM_EPOCHS = 50\n",
    "FEATURES_DISC = 256\n",
    "FEATURES_GEN = 256\n",
    "# Hyperparameters etc.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "# LEARNING_RATE_DISC = 2e-6  # could also use two lrs, one for gen and one for disc\n",
    "# LEARNING_RATE_GEN = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "# BATCH_SIZE = 5\n",
    "# IMAGE_SIZE = 64\n",
    "# # CHANNELS_IMG = 1 # MNIST or maps\n",
    "# CHANNELS_IMG = 3 # CelebA Dataset\n",
    "# NOISE_DIM = 100\n",
    "# NUM_EPOCHS = 25\n",
    "# FEATURES_DISC = 64\n",
    "# FEATURES_GEN = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToPILImage(),\n",
    "        # tfms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading paths dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "\n",
    "dataset = PathsDataset(path = \"./data/map_64x64/\", shape = (64,64), transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE_GEN, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(2048, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/2000                   Loss D: 0.7046, loss G: 0.6750\n",
      "Epoch [0/50] Batch 100/2000                   Loss D: 1.2367, loss G: 0.1834\n",
      "Epoch [0/50] Batch 200/2000                   Loss D: 1.4813, loss G: 0.1085\n",
      "Epoch [0/50] Batch 300/2000                   Loss D: 1.6102, loss G: 0.0829\n",
      "Epoch [0/50] Batch 400/2000                   Loss D: 1.6514, loss G: 0.0761\n",
      "Epoch [0/50] Batch 500/2000                   Loss D: 1.6367, loss G: 0.0774\n",
      "Epoch [0/50] Batch 600/2000                   Loss D: 1.6196, loss G: 0.0795\n",
      "Epoch [0/50] Batch 700/2000                   Loss D: 1.6001, loss G: 0.0826\n",
      "Epoch [0/50] Batch 800/2000                   Loss D: 1.5861, loss G: 0.0854\n",
      "Epoch [0/50] Batch 900/2000                   Loss D: 1.5643, loss G: 0.0886\n",
      "Epoch [0/50] Batch 1000/2000                   Loss D: 1.5418, loss G: 0.0922\n",
      "Epoch [0/50] Batch 1100/2000                   Loss D: 1.5188, loss G: 0.0971\n",
      "Epoch [0/50] Batch 1200/2000                   Loss D: 1.4850, loss G: 0.1032\n",
      "Epoch [0/50] Batch 1300/2000                   Loss D: 1.4593, loss G: 0.1085\n",
      "Epoch [0/50] Batch 1400/2000                   Loss D: 1.4386, loss G: 0.1132\n",
      "Epoch [0/50] Batch 1500/2000                   Loss D: 1.4465, loss G: 0.1116\n",
      "Epoch [0/50] Batch 1600/2000                   Loss D: 1.4246, loss G: 0.1155\n",
      "Epoch [0/50] Batch 1700/2000                   Loss D: 1.4161, loss G: 0.1181\n",
      "Epoch [0/50] Batch 1800/2000                   Loss D: 1.4042, loss G: 0.1188\n",
      "Epoch [0/50] Batch 1900/2000                   Loss D: 1.3994, loss G: 0.1218\n",
      "Epoch [1/50] Batch 0/2000                   Loss D: 1.3857, loss G: 0.1252\n",
      "Epoch [1/50] Batch 100/2000                   Loss D: 1.3761, loss G: 0.1263\n",
      "Epoch [1/50] Batch 200/2000                   Loss D: 1.3651, loss G: 0.1281\n",
      "Epoch [1/50] Batch 300/2000                   Loss D: 1.3652, loss G: 0.1283\n",
      "Epoch [1/50] Batch 400/2000                   Loss D: 1.3568, loss G: 0.1315\n",
      "Epoch [1/50] Batch 500/2000                   Loss D: 1.3383, loss G: 0.1344\n",
      "Epoch [1/50] Batch 600/2000                   Loss D: 1.3336, loss G: 0.1369\n",
      "Epoch [1/50] Batch 700/2000                   Loss D: 1.3041, loss G: 0.1447\n",
      "Epoch [1/50] Batch 800/2000                   Loss D: 1.2883, loss G: 0.1501\n",
      "Epoch [1/50] Batch 900/2000                   Loss D: 1.2723, loss G: 0.1558\n",
      "Epoch [1/50] Batch 1000/2000                   Loss D: 1.2579, loss G: 0.1579\n",
      "Epoch [1/50] Batch 1100/2000                   Loss D: 1.2680, loss G: 0.1547\n",
      "Epoch [1/50] Batch 1200/2000                   Loss D: 1.2717, loss G: 0.1530\n",
      "Epoch [1/50] Batch 1300/2000                   Loss D: 1.2650, loss G: 0.1550\n",
      "Epoch [1/50] Batch 1400/2000                   Loss D: 1.2501, loss G: 0.1593\n",
      "Epoch [1/50] Batch 1500/2000                   Loss D: 1.2364, loss G: 0.1643\n",
      "Epoch [1/50] Batch 1600/2000                   Loss D: 1.2291, loss G: 0.1659\n",
      "Epoch [1/50] Batch 1700/2000                   Loss D: 1.2160, loss G: 0.1717\n",
      "Epoch [1/50] Batch 1800/2000                   Loss D: 1.2078, loss G: 0.1730\n",
      "Epoch [1/50] Batch 1900/2000                   Loss D: 1.2019, loss G: 0.1744\n",
      "Epoch [2/50] Batch 0/2000                   Loss D: 1.2018, loss G: 0.1746\n",
      "Epoch [2/50] Batch 100/2000                   Loss D: 1.1969, loss G: 0.1763\n",
      "Epoch [2/50] Batch 200/2000                   Loss D: 1.1972, loss G: 0.1767\n",
      "Epoch [2/50] Batch 300/2000                   Loss D: 1.1852, loss G: 0.1808\n",
      "Epoch [2/50] Batch 400/2000                   Loss D: 1.1716, loss G: 0.1852\n",
      "Epoch [2/50] Batch 500/2000                   Loss D: 1.1671, loss G: 0.1891\n",
      "Epoch [2/50] Batch 600/2000                   Loss D: 1.1661, loss G: 0.1886\n",
      "Epoch [2/50] Batch 700/2000                   Loss D: 1.1658, loss G: 0.1891\n",
      "Epoch [2/50] Batch 800/2000                   Loss D: 1.1533, loss G: 0.1911\n",
      "Epoch [2/50] Batch 900/2000                   Loss D: 1.1536, loss G: 0.1926\n",
      "Epoch [2/50] Batch 1000/2000                   Loss D: 1.1404, loss G: 0.1964\n",
      "Epoch [2/50] Batch 1100/2000                   Loss D: 1.1357, loss G: 0.1996\n",
      "Epoch [2/50] Batch 1200/2000                   Loss D: 1.1296, loss G: 0.2009\n",
      "Epoch [2/50] Batch 1300/2000                   Loss D: 1.1201, loss G: 0.2039\n",
      "Epoch [2/50] Batch 1400/2000                   Loss D: 1.1173, loss G: 0.2069\n",
      "Epoch [2/50] Batch 1500/2000                   Loss D: 1.1108, loss G: 0.2084\n",
      "Epoch [2/50] Batch 1600/2000                   Loss D: 1.1057, loss G: 0.2131\n",
      "Epoch [2/50] Batch 1700/2000                   Loss D: 1.0911, loss G: 0.2175\n",
      "Epoch [2/50] Batch 1800/2000                   Loss D: 1.0764, loss G: 0.2235\n",
      "Epoch [2/50] Batch 1900/2000                   Loss D: 1.0695, loss G: 0.2296\n",
      "Epoch [3/50] Batch 0/2000                   Loss D: 1.0645, loss G: 0.2300\n",
      "Epoch [3/50] Batch 100/2000                   Loss D: 1.0609, loss G: 0.2310\n",
      "Epoch [3/50] Batch 200/2000                   Loss D: 1.0637, loss G: 0.2315\n",
      "Epoch [3/50] Batch 300/2000                   Loss D: 1.0596, loss G: 0.2314\n",
      "Epoch [3/50] Batch 400/2000                   Loss D: 1.0631, loss G: 0.2312\n",
      "Epoch [3/50] Batch 500/2000                   Loss D: 1.0545, loss G: 0.2353\n",
      "Epoch [3/50] Batch 600/2000                   Loss D: 1.0473, loss G: 0.2403\n",
      "Epoch [3/50] Batch 700/2000                   Loss D: 1.0409, loss G: 0.2404\n",
      "Epoch [3/50] Batch 800/2000                   Loss D: 1.0259, loss G: 0.2485\n",
      "Epoch [3/50] Batch 900/2000                   Loss D: 1.0273, loss G: 0.2473\n",
      "Epoch [3/50] Batch 1000/2000                   Loss D: 1.0362, loss G: 0.2418\n",
      "Epoch [3/50] Batch 1100/2000                   Loss D: 1.0339, loss G: 0.2452\n",
      "Epoch [3/50] Batch 1200/2000                   Loss D: 1.0164, loss G: 0.2519\n",
      "Epoch [3/50] Batch 1300/2000                   Loss D: 1.0065, loss G: 0.2587\n",
      "Epoch [3/50] Batch 1400/2000                   Loss D: 0.9966, loss G: 0.2647\n",
      "Epoch [3/50] Batch 1500/2000                   Loss D: 0.9972, loss G: 0.2644\n",
      "Epoch [3/50] Batch 1600/2000                   Loss D: 0.9893, loss G: 0.2692\n",
      "Epoch [3/50] Batch 1700/2000                   Loss D: 0.9832, loss G: 0.2726\n",
      "Epoch [3/50] Batch 1800/2000                   Loss D: 0.9738, loss G: 0.2774\n",
      "Epoch [3/50] Batch 1900/2000                   Loss D: 0.9756, loss G: 0.2762\n",
      "Epoch [4/50] Batch 0/2000                   Loss D: 0.9699, loss G: 0.2789\n",
      "Epoch [4/50] Batch 100/2000                   Loss D: 0.9700, loss G: 0.2775\n",
      "Epoch [4/50] Batch 200/2000                   Loss D: 0.9767, loss G: 0.2749\n",
      "Epoch [4/50] Batch 300/2000                   Loss D: 0.9728, loss G: 0.2768\n",
      "Epoch [4/50] Batch 400/2000                   Loss D: 0.9727, loss G: 0.2780\n",
      "Epoch [4/50] Batch 500/2000                   Loss D: 0.9629, loss G: 0.2813\n",
      "Epoch [4/50] Batch 600/2000                   Loss D: 0.9618, loss G: 0.2869\n",
      "Epoch [4/50] Batch 700/2000                   Loss D: 0.9541, loss G: 0.2879\n",
      "Epoch [4/50] Batch 800/2000                   Loss D: 0.9480, loss G: 0.2957\n",
      "Epoch [4/50] Batch 900/2000                   Loss D: 0.9349, loss G: 0.3034\n",
      "Epoch [4/50] Batch 1000/2000                   Loss D: 0.9236, loss G: 0.3094\n",
      "Epoch [4/50] Batch 1100/2000                   Loss D: 0.9200, loss G: 0.3113\n",
      "Epoch [4/50] Batch 1200/2000                   Loss D: 0.9172, loss G: 0.3119\n",
      "Epoch [4/50] Batch 1300/2000                   Loss D: 0.9160, loss G: 0.3122\n",
      "Epoch [4/50] Batch 1400/2000                   Loss D: 0.9129, loss G: 0.3181\n",
      "Epoch [4/50] Batch 1500/2000                   Loss D: 0.9039, loss G: 0.3242\n",
      "Epoch [4/50] Batch 1600/2000                   Loss D: 0.9034, loss G: 0.3248\n",
      "Epoch [4/50] Batch 1700/2000                   Loss D: 0.9005, loss G: 0.3265\n",
      "Epoch [4/50] Batch 1800/2000                   Loss D: 0.8955, loss G: 0.3302\n",
      "Epoch [4/50] Batch 1900/2000                   Loss D: 0.9004, loss G: 0.3259\n",
      "Epoch [5/50] Batch 0/2000                   Loss D: 0.8926, loss G: 0.3306\n",
      "Epoch [5/50] Batch 100/2000                   Loss D: 0.8855, loss G: 0.3349\n",
      "Epoch [5/50] Batch 200/2000                   Loss D: 0.8863, loss G: 0.3358\n",
      "Epoch [5/50] Batch 300/2000                   Loss D: 0.8783, loss G: 0.3434\n",
      "Epoch [5/50] Batch 400/2000                   Loss D: 0.8653, loss G: 0.3520\n",
      "Epoch [5/50] Batch 500/2000                   Loss D: 0.8625, loss G: 0.3568\n",
      "Epoch [5/50] Batch 600/2000                   Loss D: 0.8532, loss G: 0.3588\n",
      "Epoch [5/50] Batch 700/2000                   Loss D: 0.8607, loss G: 0.3556\n",
      "Epoch [5/50] Batch 800/2000                   Loss D: 0.8680, loss G: 0.3521\n",
      "Epoch [5/50] Batch 900/2000                   Loss D: 0.8668, loss G: 0.3498\n",
      "Epoch [5/50] Batch 1000/2000                   Loss D: 0.8537, loss G: 0.3594\n",
      "Epoch [5/50] Batch 1100/2000                   Loss D: 0.8472, loss G: 0.3668\n",
      "Epoch [5/50] Batch 1200/2000                   Loss D: 0.8421, loss G: 0.3723\n",
      "Epoch [5/50] Batch 1300/2000                   Loss D: 0.8419, loss G: 0.3722\n",
      "Epoch [5/50] Batch 1400/2000                   Loss D: 0.8369, loss G: 0.3781\n",
      "Epoch [5/50] Batch 1500/2000                   Loss D: 0.8271, loss G: 0.3841\n",
      "Epoch [5/50] Batch 1600/2000                   Loss D: 0.8293, loss G: 0.3853\n",
      "Epoch [5/50] Batch 1700/2000                   Loss D: 0.8211, loss G: 0.3933\n",
      "Epoch [5/50] Batch 1800/2000                   Loss D: 0.8116, loss G: 0.3983\n",
      "Epoch [5/50] Batch 1900/2000                   Loss D: 0.8099, loss G: 0.4007\n",
      "Epoch [6/50] Batch 0/2000                   Loss D: 0.8075, loss G: 0.4060\n",
      "Epoch [6/50] Batch 100/2000                   Loss D: 0.8097, loss G: 0.4036\n",
      "Epoch [6/50] Batch 200/2000                   Loss D: 0.8046, loss G: 0.4084\n",
      "Epoch [6/50] Batch 300/2000                   Loss D: 0.7976, loss G: 0.4150\n",
      "Epoch [6/50] Batch 400/2000                   Loss D: 0.7989, loss G: 0.4165\n",
      "Epoch [6/50] Batch 500/2000                   Loss D: 0.7999, loss G: 0.4122\n",
      "Epoch [6/50] Batch 600/2000                   Loss D: 0.8010, loss G: 0.4139\n",
      "Epoch [6/50] Batch 700/2000                   Loss D: 0.8064, loss G: 0.4120\n",
      "Epoch [6/50] Batch 800/2000                   Loss D: 0.7960, loss G: 0.4203\n",
      "Epoch [6/50] Batch 900/2000                   Loss D: 0.8004, loss G: 0.4194\n",
      "Epoch [6/50] Batch 1000/2000                   Loss D: 0.7991, loss G: 0.4186\n",
      "Epoch [6/50] Batch 1100/2000                   Loss D: 0.8035, loss G: 0.4217\n",
      "Epoch [6/50] Batch 1200/2000                   Loss D: 0.8227, loss G: 0.4052\n",
      "Epoch [6/50] Batch 1300/2000                   Loss D: 0.8454, loss G: 0.3885\n",
      "Epoch [6/50] Batch 1400/2000                   Loss D: 0.8409, loss G: 0.3993\n",
      "Epoch [6/50] Batch 1500/2000                   Loss D: 0.8341, loss G: 0.4160\n",
      "Epoch [6/50] Batch 1600/2000                   Loss D: 0.8259, loss G: 0.4318\n",
      "Epoch [6/50] Batch 1700/2000                   Loss D: 0.8129, loss G: 0.4505\n",
      "Epoch [6/50] Batch 1800/2000                   Loss D: 0.8090, loss G: 0.4679\n",
      "Epoch [6/50] Batch 1900/2000                   Loss D: 0.8005, loss G: 0.4856\n",
      "Epoch [7/50] Batch 0/2000                   Loss D: 0.7946, loss G: 0.5013\n",
      "Epoch [7/50] Batch 100/2000                   Loss D: 0.7885, loss G: 0.5148\n",
      "Epoch [7/50] Batch 200/2000                   Loss D: 0.7826, loss G: 0.5291\n",
      "Epoch [7/50] Batch 300/2000                   Loss D: 0.7748, loss G: 0.5451\n",
      "Epoch [7/50] Batch 400/2000                   Loss D: 0.7722, loss G: 0.5571\n",
      "Epoch [7/50] Batch 500/2000                   Loss D: 0.7682, loss G: 0.5674\n",
      "Epoch [7/50] Batch 600/2000                   Loss D: 0.7624, loss G: 0.5751\n",
      "Epoch [7/50] Batch 700/2000                   Loss D: 0.7605, loss G: 0.5828\n",
      "Epoch [7/50] Batch 800/2000                   Loss D: 0.7610, loss G: 0.5910\n",
      "Epoch [7/50] Batch 900/2000                   Loss D: 0.7561, loss G: 0.5923\n",
      "Epoch [7/50] Batch 1000/2000                   Loss D: 0.7595, loss G: 0.5947\n",
      "Epoch [7/50] Batch 1100/2000                   Loss D: 0.7524, loss G: 0.6030\n",
      "Epoch [7/50] Batch 1200/2000                   Loss D: 0.7450, loss G: 0.6138\n",
      "Epoch [7/50] Batch 1300/2000                   Loss D: 0.7389, loss G: 0.6214\n",
      "Epoch [7/50] Batch 1400/2000                   Loss D: 0.7442, loss G: 0.6175\n",
      "Epoch [7/50] Batch 1500/2000                   Loss D: 0.7424, loss G: 0.6203\n",
      "Epoch [7/50] Batch 1600/2000                   Loss D: 0.7449, loss G: 0.6220\n",
      "Epoch [7/50] Batch 1700/2000                   Loss D: 0.7396, loss G: 0.6309\n",
      "Epoch [7/50] Batch 1800/2000                   Loss D: 0.7429, loss G: 0.6300\n",
      "Epoch [7/50] Batch 1900/2000                   Loss D: 0.7481, loss G: 0.6303\n",
      "Epoch [8/50] Batch 0/2000                   Loss D: 0.7463, loss G: 0.6331\n",
      "Epoch [8/50] Batch 100/2000                   Loss D: 0.7429, loss G: 0.6379\n",
      "Epoch [8/50] Batch 200/2000                   Loss D: 0.7480, loss G: 0.6337\n",
      "Epoch [8/50] Batch 300/2000                   Loss D: 0.7404, loss G: 0.6406\n",
      "Epoch [8/50] Batch 400/2000                   Loss D: 0.7392, loss G: 0.6431\n",
      "Epoch [8/50] Batch 500/2000                   Loss D: 0.7355, loss G: 0.6436\n",
      "Epoch [8/50] Batch 600/2000                   Loss D: 0.7330, loss G: 0.6394\n",
      "Epoch [8/50] Batch 700/2000                   Loss D: 0.7378, loss G: 0.6372\n",
      "Epoch [8/50] Batch 800/2000                   Loss D: 0.7359, loss G: 0.6450\n",
      "Epoch [8/50] Batch 900/2000                   Loss D: 0.7340, loss G: 0.6475\n",
      "Epoch [8/50] Batch 1000/2000                   Loss D: 0.7338, loss G: 0.6531\n",
      "Epoch [8/50] Batch 1100/2000                   Loss D: 0.7343, loss G: 0.6539\n",
      "Epoch [8/50] Batch 1200/2000                   Loss D: 0.7305, loss G: 0.6619\n",
      "Epoch [8/50] Batch 1300/2000                   Loss D: 0.7272, loss G: 0.6680\n",
      "Epoch [8/50] Batch 1400/2000                   Loss D: 0.7272, loss G: 0.6696\n",
      "Epoch [8/50] Batch 1500/2000                   Loss D: 0.7305, loss G: 0.6660\n",
      "Epoch [8/50] Batch 1600/2000                   Loss D: 0.7329, loss G: 0.6645\n",
      "Epoch [8/50] Batch 1700/2000                   Loss D: 0.7271, loss G: 0.6709\n",
      "Epoch [8/50] Batch 1800/2000                   Loss D: 0.7271, loss G: 0.6737\n",
      "Epoch [8/50] Batch 1900/2000                   Loss D: 0.7247, loss G: 0.6742\n",
      "Epoch [9/50] Batch 0/2000                   Loss D: 0.7212, loss G: 0.6738\n",
      "Epoch [9/50] Batch 100/2000                   Loss D: 0.7187, loss G: 0.6741\n",
      "Epoch [9/50] Batch 200/2000                   Loss D: 0.7139, loss G: 0.6794\n",
      "Epoch [9/50] Batch 300/2000                   Loss D: 0.7169, loss G: 0.6744\n",
      "Epoch [9/50] Batch 400/2000                   Loss D: 0.7189, loss G: 0.6723\n",
      "Epoch [9/50] Batch 500/2000                   Loss D: 0.7222, loss G: 0.6739\n",
      "Epoch [9/50] Batch 600/2000                   Loss D: 0.7235, loss G: 0.6783\n",
      "Epoch [9/50] Batch 700/2000                   Loss D: 0.7194, loss G: 0.6851\n",
      "Epoch [9/50] Batch 800/2000                   Loss D: 0.7180, loss G: 0.6937\n",
      "Epoch [9/50] Batch 900/2000                   Loss D: 0.7147, loss G: 0.6980\n",
      "Epoch [9/50] Batch 1000/2000                   Loss D: 0.7193, loss G: 0.6913\n",
      "Epoch [9/50] Batch 1100/2000                   Loss D: 0.7202, loss G: 0.6860\n",
      "Epoch [9/50] Batch 1200/2000                   Loss D: 0.7192, loss G: 0.6821\n",
      "Epoch [9/50] Batch 1300/2000                   Loss D: 0.7175, loss G: 0.6819\n",
      "Epoch [9/50] Batch 1400/2000                   Loss D: 0.7196, loss G: 0.6740\n",
      "Epoch [9/50] Batch 1500/2000                   Loss D: 0.7120, loss G: 0.6790\n",
      "Epoch [9/50] Batch 1600/2000                   Loss D: 0.7128, loss G: 0.6734\n",
      "Epoch [9/50] Batch 1700/2000                   Loss D: 0.7195, loss G: 0.6638\n",
      "Epoch [9/50] Batch 1800/2000                   Loss D: 0.7193, loss G: 0.6718\n",
      "Epoch [9/50] Batch 1900/2000                   Loss D: 0.7184, loss G: 0.6780\n",
      "Epoch [10/50] Batch 0/2000                   Loss D: 0.7145, loss G: 0.6901\n",
      "Epoch [10/50] Batch 100/2000                   Loss D: 0.7130, loss G: 0.6984\n",
      "Epoch [10/50] Batch 200/2000                   Loss D: 0.7091, loss G: 0.7018\n",
      "Epoch [10/50] Batch 300/2000                   Loss D: 0.7073, loss G: 0.7065\n",
      "Epoch [10/50] Batch 400/2000                   Loss D: 0.7090, loss G: 0.7038\n",
      "Epoch [10/50] Batch 500/2000                   Loss D: 0.7086, loss G: 0.7030\n",
      "Epoch [10/50] Batch 600/2000                   Loss D: 0.7147, loss G: 0.6923\n",
      "Epoch [10/50] Batch 700/2000                   Loss D: 0.7081, loss G: 0.6917\n",
      "Epoch [10/50] Batch 800/2000                   Loss D: 0.7038, loss G: 0.6961\n",
      "Epoch [10/50] Batch 900/2000                   Loss D: 0.7038, loss G: 0.6953\n",
      "Epoch [10/50] Batch 1000/2000                   Loss D: 0.7061, loss G: 0.6922\n",
      "Epoch [10/50] Batch 1100/2000                   Loss D: 0.7134, loss G: 0.6760\n",
      "Epoch [10/50] Batch 1200/2000                   Loss D: 0.7125, loss G: 0.6808\n",
      "Epoch [10/50] Batch 1300/2000                   Loss D: 0.7104, loss G: 0.6879\n",
      "Epoch [10/50] Batch 1400/2000                   Loss D: 0.7112, loss G: 0.6875\n",
      "Epoch [10/50] Batch 1500/2000                   Loss D: 0.7136, loss G: 0.6846\n",
      "Epoch [10/50] Batch 1600/2000                   Loss D: 0.7076, loss G: 0.6901\n",
      "Epoch [10/50] Batch 1700/2000                   Loss D: 0.7029, loss G: 0.6907\n",
      "Epoch [10/50] Batch 1800/2000                   Loss D: 0.7045, loss G: 0.6962\n",
      "Epoch [10/50] Batch 1900/2000                   Loss D: 0.7040, loss G: 0.6939\n",
      "Epoch [11/50] Batch 0/2000                   Loss D: 0.7124, loss G: 0.6803\n",
      "Epoch [11/50] Batch 100/2000                   Loss D: 0.7077, loss G: 0.6858\n",
      "Epoch [11/50] Batch 200/2000                   Loss D: 0.7058, loss G: 0.6863\n",
      "Epoch [11/50] Batch 300/2000                   Loss D: 0.7057, loss G: 0.6926\n",
      "Epoch [11/50] Batch 400/2000                   Loss D: 0.7125, loss G: 0.6858\n",
      "Epoch [11/50] Batch 500/2000                   Loss D: 0.7066, loss G: 0.6929\n",
      "Epoch [11/50] Batch 600/2000                   Loss D: 0.7105, loss G: 0.6906\n",
      "Epoch [11/50] Batch 700/2000                   Loss D: 0.7074, loss G: 0.6976\n",
      "Epoch [11/50] Batch 800/2000                   Loss D: 0.7063, loss G: 0.7035\n",
      "Epoch [11/50] Batch 900/2000                   Loss D: 0.7074, loss G: 0.7087\n",
      "Epoch [11/50] Batch 1000/2000                   Loss D: 0.7053, loss G: 0.7118\n",
      "Epoch [11/50] Batch 1100/2000                   Loss D: 0.7051, loss G: 0.7027\n",
      "Epoch [11/50] Batch 1200/2000                   Loss D: 0.7100, loss G: 0.6880\n",
      "Epoch [11/50] Batch 1300/2000                   Loss D: 0.7066, loss G: 0.6949\n",
      "Epoch [11/50] Batch 1400/2000                   Loss D: 0.7074, loss G: 0.6886\n",
      "Epoch [11/50] Batch 1500/2000                   Loss D: 0.6990, loss G: 0.6962\n",
      "Epoch [11/50] Batch 1600/2000                   Loss D: 0.7008, loss G: 0.6997\n",
      "Epoch [11/50] Batch 1700/2000                   Loss D: 0.7025, loss G: 0.6939\n",
      "Epoch [11/50] Batch 1800/2000                   Loss D: 0.7058, loss G: 0.6945\n",
      "Epoch [11/50] Batch 1900/2000                   Loss D: 0.7074, loss G: 0.6868\n",
      "Epoch [12/50] Batch 0/2000                   Loss D: 0.7079, loss G: 0.6799\n",
      "Epoch [12/50] Batch 100/2000                   Loss D: 0.7035, loss G: 0.6840\n",
      "Epoch [12/50] Batch 200/2000                   Loss D: 0.7082, loss G: 0.6816\n",
      "Epoch [12/50] Batch 300/2000                   Loss D: 0.7021, loss G: 0.6930\n",
      "Epoch [12/50] Batch 400/2000                   Loss D: 0.7004, loss G: 0.7022\n",
      "Epoch [12/50] Batch 500/2000                   Loss D: 0.7104, loss G: 0.6831\n",
      "Epoch [12/50] Batch 600/2000                   Loss D: 0.7147, loss G: 0.6693\n",
      "Epoch [12/50] Batch 700/2000                   Loss D: 0.7104, loss G: 0.6777\n",
      "Epoch [12/50] Batch 800/2000                   Loss D: 0.7040, loss G: 0.6851\n",
      "Epoch [12/50] Batch 900/2000                   Loss D: 0.6992, loss G: 0.6948\n",
      "Epoch [12/50] Batch 1000/2000                   Loss D: 0.7073, loss G: 0.6839\n",
      "Epoch [12/50] Batch 1100/2000                   Loss D: 0.7089, loss G: 0.6766\n",
      "Epoch [12/50] Batch 1200/2000                   Loss D: 0.7074, loss G: 0.6843\n",
      "Epoch [12/50] Batch 1300/2000                   Loss D: 0.7009, loss G: 0.6934\n",
      "Epoch [12/50] Batch 1400/2000                   Loss D: 0.6997, loss G: 0.6948\n",
      "Epoch [12/50] Batch 1500/2000                   Loss D: 0.7018, loss G: 0.6921\n",
      "Epoch [12/50] Batch 1600/2000                   Loss D: 0.7001, loss G: 0.6955\n",
      "Epoch [12/50] Batch 1700/2000                   Loss D: 0.7063, loss G: 0.6850\n",
      "Epoch [12/50] Batch 1800/2000                   Loss D: 0.7137, loss G: 0.6712\n",
      "Epoch [12/50] Batch 1900/2000                   Loss D: 0.7108, loss G: 0.6757\n",
      "Epoch [13/50] Batch 0/2000                   Loss D: 0.7050, loss G: 0.6839\n",
      "Epoch [13/50] Batch 100/2000                   Loss D: 0.7008, loss G: 0.6875\n",
      "Epoch [13/50] Batch 200/2000                   Loss D: 0.6952, loss G: 0.6973\n",
      "Epoch [13/50] Batch 300/2000                   Loss D: 0.7007, loss G: 0.6914\n",
      "Epoch [13/50] Batch 400/2000                   Loss D: 0.7061, loss G: 0.6862\n",
      "Epoch [13/50] Batch 500/2000                   Loss D: 0.7048, loss G: 0.6890\n",
      "Epoch [13/50] Batch 600/2000                   Loss D: 0.6994, loss G: 0.6972\n",
      "Epoch [13/50] Batch 700/2000                   Loss D: 0.6971, loss G: 0.7020\n",
      "Epoch [13/50] Batch 800/2000                   Loss D: 0.6992, loss G: 0.7000\n",
      "Epoch [13/50] Batch 900/2000                   Loss D: 0.6971, loss G: 0.6984\n",
      "Epoch [13/50] Batch 1000/2000                   Loss D: 0.6943, loss G: 0.7014\n",
      "Epoch [13/50] Batch 1100/2000                   Loss D: 0.6972, loss G: 0.7065\n",
      "Epoch [13/50] Batch 1200/2000                   Loss D: 0.7015, loss G: 0.6955\n",
      "Epoch [13/50] Batch 1300/2000                   Loss D: 0.7010, loss G: 0.6926\n",
      "Epoch [13/50] Batch 1400/2000                   Loss D: 0.6982, loss G: 0.7004\n",
      "Epoch [13/50] Batch 1500/2000                   Loss D: 0.7033, loss G: 0.6898\n",
      "Epoch [13/50] Batch 1600/2000                   Loss D: 0.7033, loss G: 0.6876\n",
      "Epoch [13/50] Batch 1700/2000                   Loss D: 0.7009, loss G: 0.6945\n",
      "Epoch [13/50] Batch 1800/2000                   Loss D: 0.6974, loss G: 0.7014\n",
      "Epoch [13/50] Batch 1900/2000                   Loss D: 0.6954, loss G: 0.7106\n",
      "Epoch [14/50] Batch 0/2000                   Loss D: 0.6924, loss G: 0.7106\n",
      "Epoch [14/50] Batch 100/2000                   Loss D: 0.7019, loss G: 0.6968\n",
      "Epoch [14/50] Batch 200/2000                   Loss D: 0.7028, loss G: 0.6924\n",
      "Epoch [14/50] Batch 300/2000                   Loss D: 0.6996, loss G: 0.6945\n",
      "Epoch [14/50] Batch 400/2000                   Loss D: 0.6993, loss G: 0.6881\n",
      "Epoch [14/50] Batch 500/2000                   Loss D: 0.6988, loss G: 0.6967\n",
      "Epoch [14/50] Batch 600/2000                   Loss D: 0.7028, loss G: 0.6900\n",
      "Epoch [14/50] Batch 700/2000                   Loss D: 0.7048, loss G: 0.6782\n",
      "Epoch [14/50] Batch 800/2000                   Loss D: 0.7098, loss G: 0.6663\n",
      "Epoch [14/50] Batch 900/2000                   Loss D: 0.7043, loss G: 0.6818\n",
      "Epoch [14/50] Batch 1000/2000                   Loss D: 0.7067, loss G: 0.6799\n",
      "Epoch [14/50] Batch 1100/2000                   Loss D: 0.7016, loss G: 0.6895\n",
      "Epoch [14/50] Batch 1200/2000                   Loss D: 0.6983, loss G: 0.6971\n",
      "Epoch [14/50] Batch 1300/2000                   Loss D: 0.6927, loss G: 0.7023\n",
      "Epoch [14/50] Batch 1400/2000                   Loss D: 0.6993, loss G: 0.6942\n",
      "Epoch [14/50] Batch 1500/2000                   Loss D: 0.6990, loss G: 0.7022\n",
      "Epoch [14/50] Batch 1600/2000                   Loss D: 0.6935, loss G: 0.7103\n",
      "Epoch [14/50] Batch 1700/2000                   Loss D: 0.6903, loss G: 0.7097\n",
      "Epoch [14/50] Batch 1800/2000                   Loss D: 0.7005, loss G: 0.6888\n",
      "Epoch [14/50] Batch 1900/2000                   Loss D: 0.6965, loss G: 0.6957\n",
      "Epoch [15/50] Batch 0/2000                   Loss D: 0.6950, loss G: 0.6984\n",
      "Epoch [15/50] Batch 100/2000                   Loss D: 0.6995, loss G: 0.6844\n",
      "Epoch [15/50] Batch 200/2000                   Loss D: 0.6981, loss G: 0.6885\n",
      "Epoch [15/50] Batch 300/2000                   Loss D: 0.6958, loss G: 0.6969\n",
      "Epoch [15/50] Batch 400/2000                   Loss D: 0.6955, loss G: 0.7057\n",
      "Epoch [15/50] Batch 500/2000                   Loss D: 0.6970, loss G: 0.7008\n",
      "Epoch [15/50] Batch 600/2000                   Loss D: 0.6954, loss G: 0.7010\n",
      "Epoch [15/50] Batch 700/2000                   Loss D: 0.6952, loss G: 0.7047\n",
      "Epoch [15/50] Batch 800/2000                   Loss D: 0.6957, loss G: 0.6976\n",
      "Epoch [15/50] Batch 900/2000                   Loss D: 0.6920, loss G: 0.7044\n",
      "Epoch [15/50] Batch 1000/2000                   Loss D: 0.7028, loss G: 0.6830\n",
      "Epoch [15/50] Batch 1100/2000                   Loss D: 0.7060, loss G: 0.6790\n",
      "Epoch [15/50] Batch 1200/2000                   Loss D: 0.7119, loss G: 0.6611\n",
      "Epoch [15/50] Batch 1300/2000                   Loss D: 0.7049, loss G: 0.6727\n",
      "Epoch [15/50] Batch 1400/2000                   Loss D: 0.7020, loss G: 0.6777\n",
      "Epoch [15/50] Batch 1500/2000                   Loss D: 0.6966, loss G: 0.6853\n",
      "Epoch [15/50] Batch 1600/2000                   Loss D: 0.7000, loss G: 0.6841\n",
      "Epoch [15/50] Batch 1700/2000                   Loss D: 0.6986, loss G: 0.6883\n",
      "Epoch [15/50] Batch 1800/2000                   Loss D: 0.6968, loss G: 0.6940\n",
      "Epoch [15/50] Batch 1900/2000                   Loss D: 0.6931, loss G: 0.7022\n",
      "Epoch [16/50] Batch 0/2000                   Loss D: 0.6969, loss G: 0.6950\n",
      "Epoch [16/50] Batch 100/2000                   Loss D: 0.6961, loss G: 0.7003\n",
      "Epoch [16/50] Batch 200/2000                   Loss D: 0.6997, loss G: 0.6941\n",
      "Epoch [16/50] Batch 300/2000                   Loss D: 0.6978, loss G: 0.7008\n",
      "Epoch [16/50] Batch 400/2000                   Loss D: 0.6995, loss G: 0.6961\n",
      "Epoch [16/50] Batch 500/2000                   Loss D: 0.7019, loss G: 0.6893\n",
      "Epoch [16/50] Batch 600/2000                   Loss D: 0.7006, loss G: 0.6857\n",
      "Epoch [16/50] Batch 700/2000                   Loss D: 0.7010, loss G: 0.6855\n",
      "Epoch [16/50] Batch 800/2000                   Loss D: 0.6993, loss G: 0.6902\n",
      "Epoch [16/50] Batch 900/2000                   Loss D: 0.6948, loss G: 0.6988\n",
      "Epoch [16/50] Batch 1000/2000                   Loss D: 0.6911, loss G: 0.7059\n",
      "Epoch [16/50] Batch 1100/2000                   Loss D: 0.6889, loss G: 0.7101\n",
      "Epoch [16/50] Batch 1200/2000                   Loss D: 0.6865, loss G: 0.7144\n",
      "Epoch [16/50] Batch 1300/2000                   Loss D: 0.7016, loss G: 0.6962\n",
      "Epoch [16/50] Batch 1400/2000                   Loss D: 0.7023, loss G: 0.6860\n",
      "Epoch [16/50] Batch 1500/2000                   Loss D: 0.7012, loss G: 0.6866\n",
      "Epoch [16/50] Batch 1600/2000                   Loss D: 0.6983, loss G: 0.6837\n",
      "Epoch [16/50] Batch 1700/2000                   Loss D: 0.6925, loss G: 0.6937\n",
      "Epoch [16/50] Batch 1800/2000                   Loss D: 0.6935, loss G: 0.6953\n",
      "Epoch [16/50] Batch 1900/2000                   Loss D: 0.6901, loss G: 0.7023\n",
      "Epoch [17/50] Batch 0/2000                   Loss D: 0.6989, loss G: 0.6896\n",
      "Epoch [17/50] Batch 100/2000                   Loss D: 0.6962, loss G: 0.6917\n",
      "Epoch [17/50] Batch 200/2000                   Loss D: 0.6898, loss G: 0.7040\n",
      "Epoch [17/50] Batch 300/2000                   Loss D: 0.6922, loss G: 0.7034\n",
      "Epoch [17/50] Batch 400/2000                   Loss D: 0.7006, loss G: 0.6875\n",
      "Epoch [17/50] Batch 500/2000                   Loss D: 0.6932, loss G: 0.6993\n",
      "Epoch [17/50] Batch 600/2000                   Loss D: 0.6899, loss G: 0.7057\n",
      "Epoch [17/50] Batch 700/2000                   Loss D: 0.6945, loss G: 0.6951\n",
      "Epoch [17/50] Batch 800/2000                   Loss D: 0.7032, loss G: 0.6792\n",
      "Epoch [17/50] Batch 900/2000                   Loss D: 0.7031, loss G: 0.6789\n",
      "Epoch [17/50] Batch 1000/2000                   Loss D: 0.6942, loss G: 0.6977\n",
      "Epoch [17/50] Batch 1100/2000                   Loss D: 0.6930, loss G: 0.7032\n",
      "Epoch [17/50] Batch 1200/2000                   Loss D: 0.6966, loss G: 0.6915\n",
      "Epoch [17/50] Batch 1300/2000                   Loss D: 0.7055, loss G: 0.6778\n",
      "Epoch [17/50] Batch 1400/2000                   Loss D: 0.6950, loss G: 0.6939\n",
      "Epoch [17/50] Batch 1500/2000                   Loss D: 0.6883, loss G: 0.7052\n",
      "Epoch [17/50] Batch 1600/2000                   Loss D: 0.6874, loss G: 0.7124\n",
      "Epoch [17/50] Batch 1700/2000                   Loss D: 0.6850, loss G: 0.7128\n",
      "Epoch [17/50] Batch 1800/2000                   Loss D: 0.6930, loss G: 0.7022\n",
      "Epoch [17/50] Batch 1900/2000                   Loss D: 0.6934, loss G: 0.7033\n",
      "Epoch [18/50] Batch 0/2000                   Loss D: 0.7017, loss G: 0.6833\n",
      "Epoch [18/50] Batch 100/2000                   Loss D: 0.6973, loss G: 0.6893\n",
      "Epoch [18/50] Batch 200/2000                   Loss D: 0.6939, loss G: 0.6962\n",
      "Epoch [18/50] Batch 300/2000                   Loss D: 0.6931, loss G: 0.7018\n",
      "Epoch [18/50] Batch 400/2000                   Loss D: 0.6889, loss G: 0.7094\n",
      "Epoch [18/50] Batch 500/2000                   Loss D: 0.6925, loss G: 0.6999\n",
      "Epoch [18/50] Batch 600/2000                   Loss D: 0.6933, loss G: 0.7012\n",
      "Epoch [18/50] Batch 700/2000                   Loss D: 0.6922, loss G: 0.7036\n",
      "Epoch [18/50] Batch 800/2000                   Loss D: 0.6937, loss G: 0.6977\n",
      "Epoch [18/50] Batch 900/2000                   Loss D: 0.7001, loss G: 0.6810\n",
      "Epoch [18/50] Batch 1000/2000                   Loss D: 0.6975, loss G: 0.6813\n",
      "Epoch [18/50] Batch 1100/2000                   Loss D: 0.6971, loss G: 0.6877\n",
      "Epoch [18/50] Batch 1200/2000                   Loss D: 0.6990, loss G: 0.6914\n",
      "Epoch [18/50] Batch 1300/2000                   Loss D: 0.6881, loss G: 0.7008\n",
      "Epoch [18/50] Batch 1400/2000                   Loss D: 0.7054, loss G: 0.6698\n",
      "Epoch [18/50] Batch 1500/2000                   Loss D: 0.6994, loss G: 0.6840\n",
      "Epoch [18/50] Batch 1600/2000                   Loss D: 0.6927, loss G: 0.6935\n",
      "Epoch [18/50] Batch 1700/2000                   Loss D: 0.6926, loss G: 0.6975\n",
      "Epoch [18/50] Batch 1800/2000                   Loss D: 0.6929, loss G: 0.6987\n",
      "Epoch [18/50] Batch 1900/2000                   Loss D: 0.6903, loss G: 0.7035\n",
      "Epoch [19/50] Batch 0/2000                   Loss D: 0.6958, loss G: 0.6932\n",
      "Epoch [19/50] Batch 100/2000                   Loss D: 0.6928, loss G: 0.6997\n",
      "Epoch [19/50] Batch 200/2000                   Loss D: 0.6958, loss G: 0.6923\n",
      "Epoch [19/50] Batch 300/2000                   Loss D: 0.6959, loss G: 0.6878\n",
      "Epoch [19/50] Batch 400/2000                   Loss D: 0.6967, loss G: 0.6872\n",
      "Epoch [19/50] Batch 500/2000                   Loss D: 0.6913, loss G: 0.6999\n",
      "Epoch [19/50] Batch 600/2000                   Loss D: 0.6869, loss G: 0.7128\n",
      "Epoch [19/50] Batch 700/2000                   Loss D: 0.6913, loss G: 0.7048\n",
      "Epoch [19/50] Batch 800/2000                   Loss D: 0.6969, loss G: 0.6917\n",
      "Epoch [19/50] Batch 900/2000                   Loss D: 0.6902, loss G: 0.7047\n",
      "Epoch [19/50] Batch 1000/2000                   Loss D: 0.6840, loss G: 0.7180\n",
      "Epoch [19/50] Batch 1100/2000                   Loss D: 0.6867, loss G: 0.7115\n",
      "Epoch [19/50] Batch 1200/2000                   Loss D: 0.6869, loss G: 0.7152\n",
      "Epoch [19/50] Batch 1300/2000                   Loss D: 0.7032, loss G: 0.6758\n",
      "Epoch [19/50] Batch 1400/2000                   Loss D: 0.6982, loss G: 0.6824\n",
      "Epoch [19/50] Batch 1500/2000                   Loss D: 0.6921, loss G: 0.6928\n",
      "Epoch [19/50] Batch 1600/2000                   Loss D: 0.6868, loss G: 0.6996\n",
      "Epoch [19/50] Batch 1700/2000                   Loss D: 0.6911, loss G: 0.6947\n",
      "Epoch [19/50] Batch 1800/2000                   Loss D: 0.6961, loss G: 0.6880\n",
      "Epoch [19/50] Batch 1900/2000                   Loss D: 0.6950, loss G: 0.6944\n",
      "Epoch [20/50] Batch 0/2000                   Loss D: 0.6921, loss G: 0.7037\n",
      "Epoch [20/50] Batch 100/2000                   Loss D: 0.6918, loss G: 0.7040\n",
      "Epoch [20/50] Batch 200/2000                   Loss D: 0.6991, loss G: 0.6861\n",
      "Epoch [20/50] Batch 300/2000                   Loss D: 0.6982, loss G: 0.6899\n",
      "Epoch [20/50] Batch 400/2000                   Loss D: 0.6898, loss G: 0.7016\n",
      "Epoch [20/50] Batch 500/2000                   Loss D: 0.6851, loss G: 0.7126\n",
      "Epoch [20/50] Batch 600/2000                   Loss D: 0.6966, loss G: 0.6914\n",
      "Epoch [20/50] Batch 700/2000                   Loss D: 0.6967, loss G: 0.6941\n",
      "Epoch [20/50] Batch 800/2000                   Loss D: 0.6903, loss G: 0.7019\n",
      "Epoch [20/50] Batch 900/2000                   Loss D: 0.6898, loss G: 0.7021\n",
      "Epoch [20/50] Batch 1000/2000                   Loss D: 0.6921, loss G: 0.6992\n",
      "Epoch [20/50] Batch 1100/2000                   Loss D: 0.6856, loss G: 0.7103\n",
      "Epoch [20/50] Batch 1200/2000                   Loss D: 0.7009, loss G: 0.6830\n",
      "Epoch [20/50] Batch 1300/2000                   Loss D: 0.6931, loss G: 0.6928\n",
      "Epoch [20/50] Batch 1400/2000                   Loss D: 0.6851, loss G: 0.7070\n",
      "Epoch [20/50] Batch 1500/2000                   Loss D: 0.6987, loss G: 0.6943\n",
      "Epoch [20/50] Batch 1600/2000                   Loss D: 0.6857, loss G: 0.7074\n",
      "Epoch [20/50] Batch 1700/2000                   Loss D: 0.6834, loss G: 0.7155\n",
      "Epoch [20/50] Batch 1800/2000                   Loss D: 0.6795, loss G: 0.7241\n",
      "Epoch [20/50] Batch 1900/2000                   Loss D: 0.7084, loss G: 0.6628\n",
      "Epoch [21/50] Batch 0/2000                   Loss D: 0.7025, loss G: 0.6767\n",
      "Epoch [21/50] Batch 100/2000                   Loss D: 0.6955, loss G: 0.6902\n",
      "Epoch [21/50] Batch 200/2000                   Loss D: 0.6912, loss G: 0.6955\n",
      "Epoch [21/50] Batch 300/2000                   Loss D: 0.6874, loss G: 0.6947\n",
      "Epoch [21/50] Batch 400/2000                   Loss D: 0.6948, loss G: 0.6952\n",
      "Epoch [21/50] Batch 500/2000                   Loss D: 0.7070, loss G: 0.6718\n",
      "Epoch [21/50] Batch 600/2000                   Loss D: 0.7038, loss G: 0.6727\n",
      "Epoch [21/50] Batch 700/2000                   Loss D: 0.7014, loss G: 0.6766\n",
      "Epoch [21/50] Batch 800/2000                   Loss D: 0.6959, loss G: 0.6894\n",
      "Epoch [21/50] Batch 900/2000                   Loss D: 0.6904, loss G: 0.7005\n",
      "Epoch [21/50] Batch 1000/2000                   Loss D: 0.6866, loss G: 0.7078\n",
      "Epoch [21/50] Batch 1100/2000                   Loss D: 0.6898, loss G: 0.7051\n",
      "Epoch [21/50] Batch 1200/2000                   Loss D: 0.6871, loss G: 0.7119\n",
      "Epoch [21/50] Batch 1300/2000                   Loss D: 0.7005, loss G: 0.6865\n",
      "Epoch [21/50] Batch 1400/2000                   Loss D: 0.6909, loss G: 0.7031\n",
      "Epoch [21/50] Batch 1500/2000                   Loss D: 0.6946, loss G: 0.6982\n",
      "Epoch [21/50] Batch 1600/2000                   Loss D: 0.6852, loss G: 0.7097\n",
      "Epoch [21/50] Batch 1700/2000                   Loss D: 0.6842, loss G: 0.7098\n",
      "Epoch [21/50] Batch 1800/2000                   Loss D: 0.6914, loss G: 0.7023\n",
      "Epoch [21/50] Batch 1900/2000                   Loss D: 0.7002, loss G: 0.6880\n",
      "Epoch [22/50] Batch 0/2000                   Loss D: 0.6908, loss G: 0.6978\n",
      "Epoch [22/50] Batch 100/2000                   Loss D: 0.6906, loss G: 0.7051\n",
      "Epoch [22/50] Batch 200/2000                   Loss D: 0.6943, loss G: 0.6900\n",
      "Epoch [22/50] Batch 300/2000                   Loss D: 0.6884, loss G: 0.6979\n",
      "Epoch [22/50] Batch 400/2000                   Loss D: 0.6834, loss G: 0.7138\n",
      "Epoch [22/50] Batch 500/2000                   Loss D: 0.6962, loss G: 0.6894\n",
      "Epoch [22/50] Batch 600/2000                   Loss D: 0.6879, loss G: 0.7045\n",
      "Epoch [22/50] Batch 700/2000                   Loss D: 0.6968, loss G: 0.6922\n",
      "Epoch [22/50] Batch 800/2000                   Loss D: 0.7112, loss G: 0.6552\n",
      "Epoch [22/50] Batch 900/2000                   Loss D: 0.7046, loss G: 0.6710\n",
      "Epoch [22/50] Batch 1000/2000                   Loss D: 0.7027, loss G: 0.6776\n",
      "Epoch [22/50] Batch 1100/2000                   Loss D: 0.6968, loss G: 0.6897\n",
      "Epoch [22/50] Batch 1200/2000                   Loss D: 0.6884, loss G: 0.7006\n",
      "Epoch [22/50] Batch 1300/2000                   Loss D: 0.6912, loss G: 0.6988\n",
      "Epoch [22/50] Batch 1400/2000                   Loss D: 0.6924, loss G: 0.7057\n",
      "Epoch [22/50] Batch 1500/2000                   Loss D: 0.6835, loss G: 0.7224\n",
      "Epoch [22/50] Batch 1600/2000                   Loss D: 0.6901, loss G: 0.7033\n",
      "Epoch [22/50] Batch 1700/2000                   Loss D: 0.6975, loss G: 0.6938\n",
      "Epoch [22/50] Batch 1800/2000                   Loss D: 0.6954, loss G: 0.6948\n",
      "Epoch [22/50] Batch 1900/2000                   Loss D: 0.6889, loss G: 0.7056\n",
      "Epoch [23/50] Batch 0/2000                   Loss D: 0.6829, loss G: 0.7173\n",
      "Epoch [23/50] Batch 100/2000                   Loss D: 0.6926, loss G: 0.6917\n",
      "Epoch [23/50] Batch 200/2000                   Loss D: 0.6963, loss G: 0.6908\n",
      "Epoch [23/50] Batch 300/2000                   Loss D: 0.6900, loss G: 0.7051\n",
      "Epoch [23/50] Batch 400/2000                   Loss D: 0.6847, loss G: 0.7092\n",
      "Epoch [23/50] Batch 500/2000                   Loss D: 0.6892, loss G: 0.6979\n",
      "Epoch [23/50] Batch 600/2000                   Loss D: 0.6863, loss G: 0.7054\n",
      "Epoch [23/50] Batch 700/2000                   Loss D: 0.6831, loss G: 0.7104\n",
      "Epoch [23/50] Batch 800/2000                   Loss D: 0.6810, loss G: 0.7118\n",
      "Epoch [23/50] Batch 900/2000                   Loss D: 0.6989, loss G: 0.6811\n",
      "Epoch [23/50] Batch 1000/2000                   Loss D: 0.6992, loss G: 0.6799\n",
      "Epoch [23/50] Batch 1100/2000                   Loss D: 0.6923, loss G: 0.6962\n",
      "Epoch [23/50] Batch 1200/2000                   Loss D: 0.6944, loss G: 0.6901\n",
      "Epoch [23/50] Batch 1300/2000                   Loss D: 0.6929, loss G: 0.6921\n",
      "Epoch [23/50] Batch 1400/2000                   Loss D: 0.6946, loss G: 0.6866\n",
      "Epoch [23/50] Batch 1500/2000                   Loss D: 0.6932, loss G: 0.6949\n",
      "Epoch [23/50] Batch 1600/2000                   Loss D: 0.6915, loss G: 0.6912\n",
      "Epoch [23/50] Batch 1700/2000                   Loss D: 0.6977, loss G: 0.6841\n",
      "Epoch [23/50] Batch 1800/2000                   Loss D: 0.6917, loss G: 0.6942\n",
      "Epoch [23/50] Batch 1900/2000                   Loss D: 0.6836, loss G: 0.7073\n",
      "Epoch [24/50] Batch 0/2000                   Loss D: 0.6844, loss G: 0.7141\n",
      "Epoch [24/50] Batch 100/2000                   Loss D: 0.6886, loss G: 0.6980\n",
      "Epoch [24/50] Batch 200/2000                   Loss D: 0.6861, loss G: 0.7062\n",
      "Epoch [24/50] Batch 300/2000                   Loss D: 0.6839, loss G: 0.7185\n",
      "Epoch [24/50] Batch 400/2000                   Loss D: 0.6953, loss G: 0.6885\n",
      "Epoch [24/50] Batch 500/2000                   Loss D: 0.6894, loss G: 0.6999\n",
      "Epoch [24/50] Batch 600/2000                   Loss D: 0.6834, loss G: 0.7080\n",
      "Epoch [24/50] Batch 700/2000                   Loss D: 0.6850, loss G: 0.7081\n",
      "Epoch [24/50] Batch 800/2000                   Loss D: 0.6932, loss G: 0.6957\n",
      "Epoch [24/50] Batch 900/2000                   Loss D: 0.6974, loss G: 0.6807\n",
      "Epoch [24/50] Batch 1000/2000                   Loss D: 0.6973, loss G: 0.6862\n",
      "Epoch [24/50] Batch 1100/2000                   Loss D: 0.7094, loss G: 0.6618\n",
      "Epoch [24/50] Batch 1200/2000                   Loss D: 0.6998, loss G: 0.6788\n",
      "Epoch [24/50] Batch 1300/2000                   Loss D: 0.6959, loss G: 0.6844\n",
      "Epoch [24/50] Batch 1400/2000                   Loss D: 0.6911, loss G: 0.6947\n",
      "Epoch [24/50] Batch 1500/2000                   Loss D: 0.6899, loss G: 0.7039\n",
      "Epoch [24/50] Batch 1600/2000                   Loss D: 0.6894, loss G: 0.7058\n",
      "Epoch [24/50] Batch 1700/2000                   Loss D: 0.6822, loss G: 0.7227\n",
      "Epoch [24/50] Batch 1800/2000                   Loss D: 0.6909, loss G: 0.7090\n",
      "Epoch [24/50] Batch 1900/2000                   Loss D: 0.6819, loss G: 0.7131\n",
      "Epoch [25/50] Batch 0/2000                   Loss D: 0.7045, loss G: 0.6767\n",
      "Epoch [25/50] Batch 100/2000                   Loss D: 0.6951, loss G: 0.6955\n",
      "Epoch [25/50] Batch 200/2000                   Loss D: 0.6981, loss G: 0.6874\n",
      "Epoch [25/50] Batch 300/2000                   Loss D: 0.6906, loss G: 0.7005\n",
      "Epoch [25/50] Batch 400/2000                   Loss D: 0.6862, loss G: 0.7033\n",
      "Epoch [25/50] Batch 500/2000                   Loss D: 0.6909, loss G: 0.6964\n",
      "Epoch [25/50] Batch 600/2000                   Loss D: 0.6880, loss G: 0.7045\n",
      "Epoch [25/50] Batch 700/2000                   Loss D: 0.6834, loss G: 0.7143\n",
      "Epoch [25/50] Batch 800/2000                   Loss D: 0.6872, loss G: 0.7088\n",
      "Epoch [25/50] Batch 900/2000                   Loss D: 0.6851, loss G: 0.7119\n",
      "Epoch [25/50] Batch 1000/2000                   Loss D: 0.6795, loss G: 0.7224\n",
      "Epoch [25/50] Batch 1100/2000                   Loss D: 0.6803, loss G: 0.7218\n",
      "Epoch [25/50] Batch 1200/2000                   Loss D: 0.6798, loss G: 0.7188\n",
      "Epoch [25/50] Batch 1300/2000                   Loss D: 0.6957, loss G: 0.6874\n",
      "Epoch [25/50] Batch 1400/2000                   Loss D: 0.6954, loss G: 0.6941\n",
      "Epoch [25/50] Batch 1500/2000                   Loss D: 0.6945, loss G: 0.6808\n",
      "Epoch [25/50] Batch 1600/2000                   Loss D: 0.6963, loss G: 0.6822\n",
      "Epoch [25/50] Batch 1700/2000                   Loss D: 0.6998, loss G: 0.6745\n",
      "Epoch [25/50] Batch 1800/2000                   Loss D: 0.6967, loss G: 0.6794\n",
      "Epoch [25/50] Batch 1900/2000                   Loss D: 0.6961, loss G: 0.6878\n",
      "Epoch [26/50] Batch 0/2000                   Loss D: 0.6865, loss G: 0.7030\n",
      "Epoch [26/50] Batch 100/2000                   Loss D: 0.6870, loss G: 0.7051\n",
      "Epoch [26/50] Batch 200/2000                   Loss D: 0.6888, loss G: 0.7051\n",
      "Epoch [26/50] Batch 300/2000                   Loss D: 0.6957, loss G: 0.7012\n",
      "Epoch [26/50] Batch 400/2000                   Loss D: 0.6823, loss G: 0.7149\n",
      "Epoch [26/50] Batch 500/2000                   Loss D: 0.6877, loss G: 0.7012\n",
      "Epoch [26/50] Batch 600/2000                   Loss D: 0.6948, loss G: 0.6929\n",
      "Epoch [26/50] Batch 700/2000                   Loss D: 0.6925, loss G: 0.6981\n",
      "Epoch [26/50] Batch 800/2000                   Loss D: 0.6849, loss G: 0.7048\n",
      "Epoch [26/50] Batch 900/2000                   Loss D: 0.6886, loss G: 0.7019\n",
      "Epoch [26/50] Batch 1000/2000                   Loss D: 0.6822, loss G: 0.7164\n",
      "Epoch [26/50] Batch 1100/2000                   Loss D: 0.6933, loss G: 0.6929\n",
      "Epoch [26/50] Batch 1200/2000                   Loss D: 0.6912, loss G: 0.6949\n",
      "Epoch [26/50] Batch 1300/2000                   Loss D: 0.6941, loss G: 0.6916\n",
      "Epoch [26/50] Batch 1400/2000                   Loss D: 0.6851, loss G: 0.7056\n",
      "Epoch [26/50] Batch 1500/2000                   Loss D: 0.6764, loss G: 0.7208\n",
      "Epoch [26/50] Batch 1600/2000                   Loss D: 0.6776, loss G: 0.7209\n",
      "Epoch [26/50] Batch 1700/2000                   Loss D: 0.6951, loss G: 0.6893\n",
      "Epoch [26/50] Batch 1800/2000                   Loss D: 0.6887, loss G: 0.7039\n",
      "Epoch [26/50] Batch 1900/2000                   Loss D: 0.6780, loss G: 0.7185\n",
      "Epoch [27/50] Batch 0/2000                   Loss D: 0.6950, loss G: 0.6870\n",
      "Epoch [27/50] Batch 100/2000                   Loss D: 0.6987, loss G: 0.6766\n",
      "Epoch [27/50] Batch 200/2000                   Loss D: 0.6947, loss G: 0.6906\n",
      "Epoch [27/50] Batch 300/2000                   Loss D: 0.6893, loss G: 0.6952\n",
      "Epoch [27/50] Batch 400/2000                   Loss D: 0.6859, loss G: 0.7001\n",
      "Epoch [27/50] Batch 500/2000                   Loss D: 0.6911, loss G: 0.6960\n",
      "Epoch [27/50] Batch 600/2000                   Loss D: 0.6791, loss G: 0.7128\n",
      "Epoch [27/50] Batch 700/2000                   Loss D: 0.6731, loss G: 0.7220\n",
      "Epoch [27/50] Batch 800/2000                   Loss D: 0.6870, loss G: 0.6970\n",
      "Epoch [27/50] Batch 900/2000                   Loss D: 0.6892, loss G: 0.6974\n",
      "Epoch [27/50] Batch 1000/2000                   Loss D: 0.6959, loss G: 0.6797\n",
      "Epoch [27/50] Batch 1100/2000                   Loss D: 0.6963, loss G: 0.6819\n",
      "Epoch [27/50] Batch 1200/2000                   Loss D: 0.6951, loss G: 0.6873\n",
      "Epoch [27/50] Batch 1300/2000                   Loss D: 0.6896, loss G: 0.6986\n",
      "Epoch [27/50] Batch 1400/2000                   Loss D: 0.7015, loss G: 0.6771\n",
      "Epoch [27/50] Batch 1500/2000                   Loss D: 0.6904, loss G: 0.6947\n",
      "Epoch [27/50] Batch 1600/2000                   Loss D: 0.6855, loss G: 0.7064\n",
      "Epoch [27/50] Batch 1700/2000                   Loss D: 0.6994, loss G: 0.6833\n",
      "Epoch [27/50] Batch 1800/2000                   Loss D: 0.6908, loss G: 0.6967\n",
      "Epoch [27/50] Batch 1900/2000                   Loss D: 0.6853, loss G: 0.7115\n",
      "Epoch [28/50] Batch 0/2000                   Loss D: 0.6814, loss G: 0.7208\n",
      "Epoch [28/50] Batch 100/2000                   Loss D: 0.6747, loss G: 0.7264\n",
      "Epoch [28/50] Batch 200/2000                   Loss D: 0.7040, loss G: 0.6648\n",
      "Epoch [28/50] Batch 300/2000                   Loss D: 0.6985, loss G: 0.6815\n",
      "Epoch [28/50] Batch 400/2000                   Loss D: 0.6921, loss G: 0.6971\n",
      "Epoch [28/50] Batch 500/2000                   Loss D: 0.6824, loss G: 0.7120\n",
      "Epoch [28/50] Batch 600/2000                   Loss D: 0.6829, loss G: 0.7141\n",
      "Epoch [28/50] Batch 700/2000                   Loss D: 0.6760, loss G: 0.7271\n",
      "Epoch [28/50] Batch 800/2000                   Loss D: 0.7039, loss G: 0.6756\n",
      "Epoch [28/50] Batch 900/2000                   Loss D: 0.6916, loss G: 0.6973\n",
      "Epoch [28/50] Batch 1000/2000                   Loss D: 0.7000, loss G: 0.6816\n",
      "Epoch [28/50] Batch 1100/2000                   Loss D: 0.6859, loss G: 0.7033\n",
      "Epoch [28/50] Batch 1200/2000                   Loss D: 0.6809, loss G: 0.7147\n",
      "Epoch [28/50] Batch 1300/2000                   Loss D: 0.6823, loss G: 0.7192\n",
      "Epoch [28/50] Batch 1400/2000                   Loss D: 0.7017, loss G: 0.6792\n",
      "Epoch [28/50] Batch 1500/2000                   Loss D: 0.6946, loss G: 0.6898\n",
      "Epoch [28/50] Batch 1600/2000                   Loss D: 0.6874, loss G: 0.7044\n",
      "Epoch [28/50] Batch 1700/2000                   Loss D: 0.6861, loss G: 0.7106\n",
      "Epoch [28/50] Batch 1800/2000                   Loss D: 0.6842, loss G: 0.7085\n",
      "Epoch [28/50] Batch 1900/2000                   Loss D: 0.6881, loss G: 0.7021\n",
      "Epoch [29/50] Batch 0/2000                   Loss D: 0.6809, loss G: 0.7117\n",
      "Epoch [29/50] Batch 100/2000                   Loss D: 0.6822, loss G: 0.7192\n",
      "Epoch [29/50] Batch 200/2000                   Loss D: 0.6884, loss G: 0.6985\n",
      "Epoch [29/50] Batch 300/2000                   Loss D: 0.6849, loss G: 0.7000\n",
      "Epoch [29/50] Batch 400/2000                   Loss D: 0.6872, loss G: 0.7090\n",
      "Epoch [29/50] Batch 500/2000                   Loss D: 0.6835, loss G: 0.7119\n",
      "Epoch [29/50] Batch 600/2000                   Loss D: 0.6776, loss G: 0.7222\n",
      "Epoch [29/50] Batch 700/2000                   Loss D: 0.6766, loss G: 0.7245\n",
      "Epoch [29/50] Batch 800/2000                   Loss D: 0.6901, loss G: 0.7004\n",
      "Epoch [29/50] Batch 900/2000                   Loss D: 0.6888, loss G: 0.6976\n",
      "Epoch [29/50] Batch 1000/2000                   Loss D: 0.6803, loss G: 0.7122\n",
      "Epoch [29/50] Batch 1100/2000                   Loss D: 0.6840, loss G: 0.7211\n",
      "Epoch [29/50] Batch 1200/2000                   Loss D: 0.6910, loss G: 0.6859\n",
      "Epoch [29/50] Batch 1300/2000                   Loss D: 0.7015, loss G: 0.6646\n",
      "Epoch [29/50] Batch 1400/2000                   Loss D: 0.7003, loss G: 0.6857\n",
      "Epoch [29/50] Batch 1500/2000                   Loss D: 0.6969, loss G: 0.6887\n",
      "Epoch [29/50] Batch 1600/2000                   Loss D: 0.6917, loss G: 0.6907\n",
      "Epoch [29/50] Batch 1700/2000                   Loss D: 0.6939, loss G: 0.6916\n",
      "Epoch [29/50] Batch 1800/2000                   Loss D: 0.6824, loss G: 0.7103\n",
      "Epoch [29/50] Batch 1900/2000                   Loss D: 0.6745, loss G: 0.7243\n",
      "Epoch [30/50] Batch 0/2000                   Loss D: 0.6824, loss G: 0.7135\n",
      "Epoch [30/50] Batch 100/2000                   Loss D: 0.6770, loss G: 0.7199\n",
      "Epoch [30/50] Batch 200/2000                   Loss D: 0.6737, loss G: 0.7256\n",
      "Epoch [30/50] Batch 300/2000                   Loss D: 0.6855, loss G: 0.7021\n",
      "Epoch [30/50] Batch 400/2000                   Loss D: 0.6981, loss G: 0.6818\n",
      "Epoch [30/50] Batch 500/2000                   Loss D: 0.6911, loss G: 0.6917\n",
      "Epoch [30/50] Batch 600/2000                   Loss D: 0.6952, loss G: 0.6854\n",
      "Epoch [30/50] Batch 700/2000                   Loss D: 0.6830, loss G: 0.7065\n",
      "Epoch [30/50] Batch 800/2000                   Loss D: 0.6788, loss G: 0.7143\n",
      "Epoch [30/50] Batch 900/2000                   Loss D: 0.6880, loss G: 0.7030\n",
      "Epoch [30/50] Batch 1000/2000                   Loss D: 0.6966, loss G: 0.6834\n",
      "Epoch [30/50] Batch 1100/2000                   Loss D: 0.6905, loss G: 0.6942\n",
      "Epoch [30/50] Batch 1200/2000                   Loss D: 0.6819, loss G: 0.7102\n",
      "Epoch [30/50] Batch 1300/2000                   Loss D: 0.6744, loss G: 0.7256\n",
      "Epoch [30/50] Batch 1400/2000                   Loss D: 0.6726, loss G: 0.7285\n",
      "Epoch [30/50] Batch 1500/2000                   Loss D: 0.6946, loss G: 0.6788\n",
      "Epoch [30/50] Batch 1600/2000                   Loss D: 0.6973, loss G: 0.6760\n",
      "Epoch [30/50] Batch 1700/2000                   Loss D: 0.6934, loss G: 0.6811\n",
      "Epoch [30/50] Batch 1800/2000                   Loss D: 0.6896, loss G: 0.6864\n",
      "Epoch [30/50] Batch 1900/2000                   Loss D: 0.6850, loss G: 0.7025\n",
      "Epoch [31/50] Batch 0/2000                   Loss D: 0.6808, loss G: 0.7132\n",
      "Epoch [31/50] Batch 100/2000                   Loss D: 0.6861, loss G: 0.7031\n",
      "Epoch [31/50] Batch 200/2000                   Loss D: 0.6987, loss G: 0.6791\n",
      "Epoch [31/50] Batch 300/2000                   Loss D: 0.6969, loss G: 0.6816\n",
      "Epoch [31/50] Batch 400/2000                   Loss D: 0.6904, loss G: 0.6967\n",
      "Epoch [31/50] Batch 500/2000                   Loss D: 0.6833, loss G: 0.7154\n",
      "Epoch [31/50] Batch 600/2000                   Loss D: 0.6829, loss G: 0.7148\n",
      "Epoch [31/50] Batch 700/2000                   Loss D: 0.6794, loss G: 0.7193\n",
      "Epoch [31/50] Batch 800/2000                   Loss D: 0.6974, loss G: 0.6814\n",
      "Epoch [31/50] Batch 900/2000                   Loss D: 0.6932, loss G: 0.6857\n",
      "Epoch [31/50] Batch 1000/2000                   Loss D: 0.6853, loss G: 0.7080\n",
      "Epoch [31/50] Batch 1100/2000                   Loss D: 0.6922, loss G: 0.6974\n",
      "Epoch [31/50] Batch 1200/2000                   Loss D: 0.6897, loss G: 0.6969\n",
      "Epoch [31/50] Batch 1300/2000                   Loss D: 0.6823, loss G: 0.7070\n",
      "Epoch [31/50] Batch 1400/2000                   Loss D: 0.6763, loss G: 0.7232\n",
      "Epoch [31/50] Batch 1500/2000                   Loss D: 0.6907, loss G: 0.7129\n",
      "Epoch [31/50] Batch 1600/2000                   Loss D: 0.6766, loss G: 0.7248\n",
      "Epoch [31/50] Batch 1700/2000                   Loss D: 0.6862, loss G: 0.7112\n",
      "Epoch [31/50] Batch 1800/2000                   Loss D: 0.7027, loss G: 0.6808\n",
      "Epoch [31/50] Batch 1900/2000                   Loss D: 0.6944, loss G: 0.6834\n",
      "Epoch [32/50] Batch 0/2000                   Loss D: 0.6876, loss G: 0.6980\n",
      "Epoch [32/50] Batch 100/2000                   Loss D: 0.6996, loss G: 0.6757\n",
      "Epoch [32/50] Batch 200/2000                   Loss D: 0.6939, loss G: 0.6868\n",
      "Epoch [32/50] Batch 300/2000                   Loss D: 0.6855, loss G: 0.7055\n",
      "Epoch [32/50] Batch 400/2000                   Loss D: 0.6894, loss G: 0.7115\n",
      "Epoch [32/50] Batch 500/2000                   Loss D: 0.7014, loss G: 0.6722\n",
      "Epoch [32/50] Batch 600/2000                   Loss D: 0.6985, loss G: 0.6721\n",
      "Epoch [32/50] Batch 700/2000                   Loss D: 0.6919, loss G: 0.6894\n",
      "Epoch [32/50] Batch 800/2000                   Loss D: 0.6837, loss G: 0.7004\n",
      "Epoch [32/50] Batch 900/2000                   Loss D: 0.6791, loss G: 0.7175\n",
      "Epoch [32/50] Batch 1000/2000                   Loss D: 0.6831, loss G: 0.7136\n",
      "Epoch [32/50] Batch 1100/2000                   Loss D: 0.6814, loss G: 0.7123\n",
      "Epoch [32/50] Batch 1200/2000                   Loss D: 0.6816, loss G: 0.7155\n",
      "Epoch [32/50] Batch 1300/2000                   Loss D: 0.6796, loss G: 0.7279\n",
      "Epoch [32/50] Batch 1400/2000                   Loss D: 0.6861, loss G: 0.7082\n",
      "Epoch [32/50] Batch 1500/2000                   Loss D: 0.6825, loss G: 0.7173\n",
      "Epoch [32/50] Batch 1600/2000                   Loss D: 0.6789, loss G: 0.7298\n",
      "Epoch [32/50] Batch 1700/2000                   Loss D: 0.6702, loss G: 0.7457\n",
      "Epoch [32/50] Batch 1800/2000                   Loss D: 0.6651, loss G: 0.7549\n",
      "Epoch [32/50] Batch 1900/2000                   Loss D: 0.6831, loss G: 0.7156\n",
      "Epoch [33/50] Batch 0/2000                   Loss D: 0.6997, loss G: 0.6770\n",
      "Epoch [33/50] Batch 100/2000                   Loss D: 0.6921, loss G: 0.6921\n",
      "Epoch [33/50] Batch 200/2000                   Loss D: 0.6823, loss G: 0.7099\n",
      "Epoch [33/50] Batch 300/2000                   Loss D: 0.6779, loss G: 0.7152\n",
      "Epoch [33/50] Batch 400/2000                   Loss D: 0.6844, loss G: 0.7015\n",
      "Epoch [33/50] Batch 500/2000                   Loss D: 0.6926, loss G: 0.6806\n",
      "Epoch [33/50] Batch 600/2000                   Loss D: 0.6858, loss G: 0.6941\n",
      "Epoch [33/50] Batch 700/2000                   Loss D: 0.6795, loss G: 0.7079\n",
      "Epoch [33/50] Batch 800/2000                   Loss D: 0.6696, loss G: 0.7241\n",
      "Epoch [33/50] Batch 900/2000                   Loss D: 0.6767, loss G: 0.7200\n",
      "Epoch [33/50] Batch 1000/2000                   Loss D: 0.6828, loss G: 0.7093\n",
      "Epoch [33/50] Batch 1100/2000                   Loss D: 0.6798, loss G: 0.7213\n",
      "Epoch [33/50] Batch 1200/2000                   Loss D: 0.6664, loss G: 0.7358\n",
      "Epoch [33/50] Batch 1300/2000                   Loss D: 0.6639, loss G: 0.7437\n",
      "Epoch [33/50] Batch 1400/2000                   Loss D: 0.6687, loss G: 0.7463\n",
      "Epoch [33/50] Batch 1500/2000                   Loss D: 0.6700, loss G: 0.7236\n",
      "Epoch [33/50] Batch 1600/2000                   Loss D: 0.6904, loss G: 0.6835\n",
      "Epoch [33/50] Batch 1700/2000                   Loss D: 0.6959, loss G: 0.6720\n",
      "Epoch [33/50] Batch 1800/2000                   Loss D: 0.6939, loss G: 0.6767\n",
      "Epoch [33/50] Batch 1900/2000                   Loss D: 0.6911, loss G: 0.6805\n",
      "Epoch [34/50] Batch 0/2000                   Loss D: 0.6813, loss G: 0.7031\n",
      "Epoch [34/50] Batch 100/2000                   Loss D: 0.6788, loss G: 0.7093\n",
      "Epoch [34/50] Batch 200/2000                   Loss D: 0.6771, loss G: 0.7173\n",
      "Epoch [34/50] Batch 300/2000                   Loss D: 0.6817, loss G: 0.7012\n",
      "Epoch [34/50] Batch 400/2000                   Loss D: 0.6825, loss G: 0.7023\n",
      "Epoch [34/50] Batch 500/2000                   Loss D: 0.6702, loss G: 0.7253\n",
      "Epoch [34/50] Batch 600/2000                   Loss D: 0.6656, loss G: 0.7356\n",
      "Epoch [34/50] Batch 700/2000                   Loss D: 0.6944, loss G: 0.6787\n",
      "Epoch [34/50] Batch 800/2000                   Loss D: 0.6881, loss G: 0.7013\n",
      "Epoch [34/50] Batch 900/2000                   Loss D: 0.6710, loss G: 0.7224\n",
      "Epoch [34/50] Batch 1000/2000                   Loss D: 0.7011, loss G: 0.6737\n",
      "Epoch [34/50] Batch 1100/2000                   Loss D: 0.6925, loss G: 0.6806\n",
      "Epoch [34/50] Batch 1200/2000                   Loss D: 0.6931, loss G: 0.6849\n",
      "Epoch [34/50] Batch 1300/2000                   Loss D: 0.6876, loss G: 0.6952\n",
      "Epoch [34/50] Batch 1400/2000                   Loss D: 0.6839, loss G: 0.6989\n",
      "Epoch [34/50] Batch 1500/2000                   Loss D: 0.6755, loss G: 0.7144\n",
      "Epoch [34/50] Batch 1600/2000                   Loss D: 0.6798, loss G: 0.7120\n",
      "Epoch [34/50] Batch 1700/2000                   Loss D: 0.6803, loss G: 0.7163\n",
      "Epoch [34/50] Batch 1800/2000                   Loss D: 0.6710, loss G: 0.7241\n",
      "Epoch [34/50] Batch 1900/2000                   Loss D: 0.6960, loss G: 0.6641\n",
      "Epoch [35/50] Batch 0/2000                   Loss D: 0.7061, loss G: 0.6636\n",
      "Epoch [35/50] Batch 100/2000                   Loss D: 0.6995, loss G: 0.6820\n",
      "Epoch [35/50] Batch 200/2000                   Loss D: 0.6889, loss G: 0.6864\n",
      "Epoch [35/50] Batch 300/2000                   Loss D: 0.6950, loss G: 0.6811\n",
      "Epoch [35/50] Batch 400/2000                   Loss D: 0.6892, loss G: 0.6973\n",
      "Epoch [35/50] Batch 500/2000                   Loss D: 0.6806, loss G: 0.7069\n",
      "Epoch [35/50] Batch 600/2000                   Loss D: 0.6713, loss G: 0.7261\n",
      "Epoch [35/50] Batch 700/2000                   Loss D: 0.6945, loss G: 0.6857\n",
      "Epoch [35/50] Batch 800/2000                   Loss D: 0.6854, loss G: 0.7038\n",
      "Epoch [35/50] Batch 900/2000                   Loss D: 0.6809, loss G: 0.7151\n",
      "Epoch [35/50] Batch 1000/2000                   Loss D: 0.6857, loss G: 0.7060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_816/2064751475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for batch_idx, (real, _) in enumerate(dataloader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real.float()).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "fake = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake.cpu().detach().numpy()[7][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bayesianNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c0cf972fe55eaf0c962c4929f592d86a72c532b00283f932a90435beee88e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
