{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, transform=None, shape = (100,100)):\n",
    "        print(\"Loading paths dataset...\")\n",
    "        # Read in path files\n",
    "        # Convert to x by y np arrays\n",
    "        # add the np arrays to a list\n",
    "        # set self.transform and self.data\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "                \n",
    "                # xvales which to interpolate on\n",
    "                # want to interpolate on xvalues from the min xval in the path to the largest xval in the path\n",
    "                self.xvals = np.linspace(int(min(self.path[:,0])), int(max(self.path[:,0])), int(max(self.path[:,0])-min(self.path[:,0])))\n",
    "                self.xvals = self.xvals.astype(int)\n",
    "\n",
    "                # interpolate for all xvals using the paths from file's x and y values\n",
    "                self.interp_path = np.interp(self.xvals, self.path[:,0], self.path[:,1])\n",
    "                self.interp_path = np.array(self.interp_path).astype(int)\n",
    "\n",
    "                # create a LxW matrix where all the values where path is equal to 1\n",
    "                self.path_matrix = np.zeros(shape)\n",
    "                self.path_matrix[self.interp_path, self.xvals] = 1\n",
    "                \n",
    "\n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"getitem\")\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "        # imagePath = self.paths_file + \"/\" + self.data['Image_path'][idx]\n",
    "        # image = sk.imread(imagePath)\n",
    "        # label = self.data['Condition'][idx]\n",
    "        # image = Image.fromarray(image)\n",
    "\n",
    "        # if self.sourceTransform:\n",
    "        #     image = self.sourceTransform(image)\n",
    "        x = np.float32(self.paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters etc.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# LEARNING_RATE_DISC = 1e-7  # could also use two lrs, one for gen and one for disc\n",
    "# LEARNING_RATE_GEN = 1e-4  # could also use two lrs, one for gen and one for disc\n",
    "# BATCH_SIZE = 10\n",
    "# IMAGE_SIZE = 256\n",
    "# CHANNELS_IMG = 1\n",
    "# NOISE_DIM = 128\n",
    "# NUM_EPOCHS = 50\n",
    "# FEATURES_DISC = 256\n",
    "# FEATURES_GEN = 256\n",
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# LEARNING_RATE_DISC = 2e-6  # could also use two lrs, one for gen and one for disc\n",
    "# LEARNING_RATE_GEN = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "# BATCH_SIZE = 5\n",
    "# NOISE_DIM = 100\n",
    "# NUM_EPOCHS = 25\n",
    "# FEATURES_DISC = 64\n",
    "# FEATURES_GEN = 64\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1 # MNIST or maps\n",
    "# CHANNELS_IMG = 3 # CelebA Dataset\n",
    "\n",
    "MAX_DATA_POINTS = 100\n",
    "MAX_IMG_DATA = 10\n",
    "\n",
    "parameters = dict(\n",
    "    lr_disc = [2e-6],\n",
    "    lr_gen = [2e-4],\n",
    "    batch_size = [10],\n",
    "    num_epochs = [25],\n",
    "    noise_dim = [100],\n",
    "    features_disc = [64],\n",
    "    features_gen = [64]\n",
    ")\n",
    "param_values = [v for v in parameters.values()]\n",
    "total_param_runs = np.prod([len(v) for v in parameters.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToPILImage(),\n",
    "        # tfms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)\n",
    "\n",
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "dataset = PathsDataset(path = \"./data/map_64x64/\", shape = (64,64), transform=transforms)\n",
    "\n",
    "# dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 0\n",
    "\n",
    "for (lr_disc, lr_gen, batch_size, num_epochs, noise_dim, features_disc, features_gen) in product(*param_values):\n",
    "\n",
    "    writer = SummaryWriter(f\"logs/run{run_number}\")\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    gen = Generator(noise_dim, CHANNELS_IMG, features_gen).to(device)\n",
    "    disc = Discriminator(CHANNELS_IMG, features_disc).to(device)\n",
    "    initialize_weights(gen)\n",
    "    initialize_weights(disc)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(32, noise_dim, 1, 1).to(device)\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    loss_step_rate = round((len(dataloader)*num_epochs)/MAX_DATA_POINTS)\n",
    "    img_step_rate = round((len(dataloader)*num_epochs)/MAX_IMG_DATA)\n",
    "\n",
    "    step = 0\n",
    "    loss_step = 0\n",
    "    img_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Target labels not needed! <3 unsupervised\n",
    "        # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        for batch_idx, real in enumerate(dataloader):\n",
    "            real = real.to(device)\n",
    "            noise = torch.randn(batch_size, noise_dim, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            disc_real = disc(real.float()).reshape(-1)\n",
    "            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            disc_fake = disc(fake.detach()).reshape(-1)\n",
    "            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "            disc.zero_grad()\n",
    "            loss_disc.backward()\n",
    "            opt_disc.step()\n",
    "\n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            output = disc(fake).reshape(-1)\n",
    "            loss_gen = criterion(output, torch.ones_like(output))\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "            # printing loss to tensorboard\n",
    "            if step % loss_step_rate == 0:\n",
    "                writer.add_scalar(f\"Discriminator Loss\", loss_disc, loss_step)\n",
    "                writer.add_scalar(f\"Generator Loss\", loss_gen, loss_step)\n",
    "                loss_step += 1\n",
    "\n",
    "            # printing image data to tensorboard\n",
    "            if step % img_step_rate == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                    Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    fake = gen(fixed_noise)\n",
    "                    # take out (up to) 32 examples\n",
    "                    img_grid_real = torchvision.utils.make_grid(\n",
    "                        real[:batch_size], normalize=True\n",
    "                    )\n",
    "                    img_grid_fake = torchvision.utils.make_grid(\n",
    "                        fake[:batch_size], normalize=True\n",
    "                    )\n",
    "\n",
    "                    writer.add_image(\"Real\", img_grid_real, global_step=img_step)\n",
    "                    writer.add_image(\"Fake\", img_grid_fake, global_step=img_step)\n",
    "\n",
    "                img_step += 1\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    #saving hyperparams:\n",
    "    writer.add_hparams({\"features_gen\": features_gen, \"features_disc\": features_disc, \"noise_dim\": noise_dim, \"lr_gen\": lr_gen, \"lr_disc\": lr_disc, \"batch_size\": batch_size, \"epochs\": num_epochs}, {\"gen loss\": loss_gen}, run_name=f\"run{run_number}\")\n",
    "    writer.close()\n",
    "    run_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "# fake = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(fake.cpu().detach().numpy()[7][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
