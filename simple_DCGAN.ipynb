{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, transform=None, shape = (100,100)):\n",
    "        print(\"Loading paths dataset...\")\n",
    "        # Read in path files\n",
    "        # Convert to x by y np arrays\n",
    "        # add the np arrays to a list\n",
    "        # set self.transform and self.data\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "                \n",
    "                # xvales which to interpolate on\n",
    "                # want to interpolate on xvalues from the min xval in the path to the largest xval in the path\n",
    "                self.xvals = np.linspace(int(min(self.path[:,0])), int(max(self.path[:,0])), int(max(self.path[:,0])-min(self.path[:,0])))\n",
    "                self.xvals = self.xvals.astype(int)\n",
    "\n",
    "                # interpolate for all xvals using the paths from file's x and y values\n",
    "                self.interp_path = np.interp(self.xvals, self.path[:,0], self.path[:,1])\n",
    "                self.interp_path = np.array(self.interp_path).astype(int)\n",
    "\n",
    "                # create a LxW matrix where all the values where path is equal to 1\n",
    "                self.path_matrix = np.zeros(shape)\n",
    "                self.path_matrix[self.interp_path, self.xvals] = 1\n",
    "                \n",
    "\n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"getitem\")\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "        # imagePath = self.paths_file + \"/\" + self.data['Image_path'][idx]\n",
    "        # image = sk.imread(imagePath)\n",
    "        # label = self.data['Condition'][idx]\n",
    "        # image = Image.fromarray(image)\n",
    "\n",
    "        # if self.sourceTransform:\n",
    "        #     image = self.sourceTransform(image)\n",
    "        x = np.float32(self.paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE_DISC = 1e-7  # could also use two lrs, one for gen and one for disc\n",
    "LEARNING_RATE_GEN = 1e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 128\n",
    "NUM_EPOCHS = 50\n",
    "FEATURES_DISC = 256\n",
    "FEATURES_GEN = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        tfms.ToPILImage(),\n",
    "        tfms.Resize(IMAGE_SIZE),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading paths dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# comment mnist above and uncomment below if train on CelebA\n",
    "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "\n",
    "dataset = PathsDataset(path = \"./data/map_20x20/\", shape = (400,400), transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE_GEN, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(2048, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1000                   Loss D: 0.7247, loss G: 1.2778\n",
      "Epoch [0/50] Batch 100/1000                   Loss D: 3.1421, loss G: 0.0046\n",
      "Epoch [0/50] Batch 200/1000                   Loss D: 2.8166, loss G: 0.0084\n",
      "Epoch [0/50] Batch 300/1000                   Loss D: 0.6645, loss G: 0.6841\n",
      "Epoch [0/50] Batch 400/1000                   Loss D: 0.5238, loss G: 0.8003\n",
      "Epoch [0/50] Batch 500/1000                   Loss D: 0.3570, loss G: 1.1436\n",
      "Epoch [0/50] Batch 600/1000                   Loss D: 0.2326, loss G: 1.5630\n",
      "Epoch [0/50] Batch 700/1000                   Loss D: 0.2031, loss G: 1.5577\n",
      "Epoch [0/50] Batch 800/1000                   Loss D: 0.2522, loss G: 1.2911\n",
      "Epoch [0/50] Batch 900/1000                   Loss D: 0.1977, loss G: 1.6335\n",
      "Epoch [1/50] Batch 0/1000                   Loss D: 0.1481, loss G: 1.9365\n",
      "Epoch [1/50] Batch 100/1000                   Loss D: 0.1179, loss G: 2.1686\n",
      "Epoch [1/50] Batch 200/1000                   Loss D: 0.0950, loss G: 2.3692\n",
      "Epoch [1/50] Batch 300/1000                   Loss D: 0.0935, loss G: 2.3073\n",
      "Epoch [1/50] Batch 400/1000                   Loss D: 0.2458, loss G: 1.2652\n",
      "Epoch [1/50] Batch 500/1000                   Loss D: 0.3219, loss G: 1.1400\n",
      "Epoch [1/50] Batch 600/1000                   Loss D: 0.2206, loss G: 1.6227\n",
      "Epoch [1/50] Batch 700/1000                   Loss D: 0.2082, loss G: 1.6487\n",
      "Epoch [1/50] Batch 800/1000                   Loss D: 0.1680, loss G: 1.8351\n",
      "Epoch [1/50] Batch 900/1000                   Loss D: 0.1400, loss G: 1.9980\n",
      "Epoch [2/50] Batch 0/1000                   Loss D: 0.1119, loss G: 2.2471\n",
      "Epoch [2/50] Batch 100/1000                   Loss D: 0.0912, loss G: 2.4545\n",
      "Epoch [2/50] Batch 200/1000                   Loss D: 0.1015, loss G: 2.2585\n",
      "Epoch [2/50] Batch 300/1000                   Loss D: 0.1212, loss G: 2.0962\n",
      "Epoch [2/50] Batch 400/1000                   Loss D: 0.0973, loss G: 2.3942\n",
      "Epoch [2/50] Batch 500/1000                   Loss D: 0.0939, loss G: 2.3649\n",
      "Epoch [2/50] Batch 600/1000                   Loss D: 0.1024, loss G: 2.2651\n",
      "Epoch [2/50] Batch 700/1000                   Loss D: 0.0826, loss G: 2.5677\n",
      "Epoch [2/50] Batch 800/1000                   Loss D: 0.0672, loss G: 2.7855\n",
      "Epoch [2/50] Batch 900/1000                   Loss D: 0.0589, loss G: 2.8781\n",
      "Epoch [3/50] Batch 0/1000                   Loss D: 0.0486, loss G: 3.0980\n",
      "Epoch [3/50] Batch 100/1000                   Loss D: 0.0472, loss G: 3.0471\n",
      "Epoch [3/50] Batch 200/1000                   Loss D: 0.0432, loss G: 3.1657\n",
      "Epoch [3/50] Batch 300/1000                   Loss D: 0.0360, loss G: 3.3741\n",
      "Epoch [3/50] Batch 400/1000                   Loss D: 0.0383, loss G: 3.2124\n",
      "Epoch [3/50] Batch 500/1000                   Loss D: 0.0351, loss G: 3.3283\n",
      "Epoch [3/50] Batch 600/1000                   Loss D: 0.0324, loss G: 3.4247\n",
      "Epoch [3/50] Batch 700/1000                   Loss D: 0.0312, loss G: 3.4441\n",
      "Epoch [3/50] Batch 800/1000                   Loss D: 0.0286, loss G: 3.5507\n",
      "Epoch [3/50] Batch 900/1000                   Loss D: 0.0268, loss G: 3.6040\n",
      "Epoch [4/50] Batch 0/1000                   Loss D: 0.0266, loss G: 3.5909\n",
      "Epoch [4/50] Batch 100/1000                   Loss D: 0.0267, loss G: 3.5877\n",
      "Epoch [4/50] Batch 200/1000                   Loss D: 0.0247, loss G: 3.6692\n",
      "Epoch [4/50] Batch 300/1000                   Loss D: 0.0233, loss G: 3.7449\n",
      "Epoch [4/50] Batch 400/1000                   Loss D: 0.0226, loss G: 3.7547\n",
      "Epoch [4/50] Batch 500/1000                   Loss D: 0.0205, loss G: 3.8881\n",
      "Epoch [4/50] Batch 600/1000                   Loss D: 0.0191, loss G: 3.9440\n",
      "Epoch [4/50] Batch 700/1000                   Loss D: 0.0188, loss G: 3.9502\n",
      "Epoch [4/50] Batch 800/1000                   Loss D: 0.0163, loss G: 4.1040\n",
      "Epoch [4/50] Batch 900/1000                   Loss D: 0.0151, loss G: 4.1796\n",
      "Epoch [5/50] Batch 0/1000                   Loss D: 0.0132, loss G: 4.3232\n",
      "Epoch [5/50] Batch 100/1000                   Loss D: 0.0118, loss G: 4.4409\n",
      "Epoch [5/50] Batch 200/1000                   Loss D: 0.0114, loss G: 4.4327\n",
      "Epoch [5/50] Batch 300/1000                   Loss D: 0.0106, loss G: 4.5177\n",
      "Epoch [5/50] Batch 400/1000                   Loss D: 0.0096, loss G: 4.6293\n",
      "Epoch [5/50] Batch 500/1000                   Loss D: 0.0089, loss G: 4.7077\n",
      "Epoch [5/50] Batch 600/1000                   Loss D: 0.0082, loss G: 4.7783\n",
      "Epoch [5/50] Batch 700/1000                   Loss D: 0.0074, loss G: 4.8818\n",
      "Epoch [5/50] Batch 800/1000                   Loss D: 0.0067, loss G: 4.9927\n",
      "Epoch [5/50] Batch 900/1000                   Loss D: 0.0060, loss G: 5.1198\n",
      "Epoch [6/50] Batch 0/1000                   Loss D: 0.0055, loss G: 5.1958\n",
      "Epoch [6/50] Batch 100/1000                   Loss D: 0.0050, loss G: 5.2753\n",
      "Epoch [6/50] Batch 200/1000                   Loss D: 0.0046, loss G: 5.3528\n",
      "Epoch [6/50] Batch 300/1000                   Loss D: 0.0042, loss G: 5.4647\n",
      "Epoch [6/50] Batch 400/1000                   Loss D: 0.0039, loss G: 5.5267\n",
      "Epoch [6/50] Batch 500/1000                   Loss D: 0.0036, loss G: 5.6133\n",
      "Epoch [6/50] Batch 600/1000                   Loss D: 0.0034, loss G: 5.6769\n",
      "Epoch [6/50] Batch 700/1000                   Loss D: 0.0032, loss G: 5.7141\n",
      "Epoch [6/50] Batch 800/1000                   Loss D: 0.0030, loss G: 5.8014\n",
      "Epoch [6/50] Batch 900/1000                   Loss D: 0.0028, loss G: 5.8747\n",
      "Epoch [7/50] Batch 0/1000                   Loss D: 0.0026, loss G: 5.9478\n",
      "Epoch [7/50] Batch 100/1000                   Loss D: 0.0024, loss G: 6.0316\n",
      "Epoch [7/50] Batch 200/1000                   Loss D: 0.0022, loss G: 6.1017\n",
      "Epoch [7/50] Batch 300/1000                   Loss D: 0.0021, loss G: 6.1578\n",
      "Epoch [7/50] Batch 400/1000                   Loss D: 0.0020, loss G: 6.1563\n",
      "Epoch [7/50] Batch 500/1000                   Loss D: 0.0019, loss G: 6.2410\n",
      "Epoch [7/50] Batch 600/1000                   Loss D: 0.0018, loss G: 6.2924\n",
      "Epoch [7/50] Batch 700/1000                   Loss D: 0.0017, loss G: 6.3589\n",
      "Epoch [7/50] Batch 800/1000                   Loss D: 0.0016, loss G: 6.4268\n",
      "Epoch [7/50] Batch 900/1000                   Loss D: 0.0015, loss G: 6.4945\n",
      "Epoch [8/50] Batch 0/1000                   Loss D: 0.0014, loss G: 6.5476\n",
      "Epoch [8/50] Batch 100/1000                   Loss D: 0.0013, loss G: 6.6276\n",
      "Epoch [8/50] Batch 200/1000                   Loss D: 0.0012, loss G: 6.6781\n",
      "Epoch [8/50] Batch 300/1000                   Loss D: 0.0012, loss G: 6.7503\n",
      "Epoch [8/50] Batch 400/1000                   Loss D: 0.0011, loss G: 6.8161\n",
      "Epoch [8/50] Batch 500/1000                   Loss D: 0.0010, loss G: 6.8718\n",
      "Epoch [8/50] Batch 600/1000                   Loss D: 0.0010, loss G: 6.9235\n",
      "Epoch [8/50] Batch 700/1000                   Loss D: 0.0009, loss G: 6.9633\n",
      "Epoch [8/50] Batch 800/1000                   Loss D: 0.0009, loss G: 6.9992\n",
      "Epoch [8/50] Batch 900/1000                   Loss D: 0.0008, loss G: 7.0670\n",
      "Epoch [9/50] Batch 0/1000                   Loss D: 0.0008, loss G: 7.1325\n",
      "Epoch [9/50] Batch 100/1000                   Loss D: 0.0007, loss G: 7.1933\n",
      "Epoch [9/50] Batch 200/1000                   Loss D: 0.0007, loss G: 7.2541\n",
      "Epoch [9/50] Batch 300/1000                   Loss D: 0.0007, loss G: 7.2829\n",
      "Epoch [9/50] Batch 400/1000                   Loss D: 0.0006, loss G: 7.3438\n",
      "Epoch [9/50] Batch 500/1000                   Loss D: 0.0006, loss G: 7.3898\n",
      "Epoch [9/50] Batch 600/1000                   Loss D: 0.0006, loss G: 7.4148\n",
      "Epoch [9/50] Batch 700/1000                   Loss D: 0.0005, loss G: 7.5036\n",
      "Epoch [9/50] Batch 800/1000                   Loss D: 0.0005, loss G: 7.5655\n",
      "Epoch [9/50] Batch 900/1000                   Loss D: 0.0005, loss G: 7.6243\n",
      "Epoch [10/50] Batch 0/1000                   Loss D: 0.0005, loss G: 7.6851\n",
      "Epoch [10/50] Batch 100/1000                   Loss D: 0.0004, loss G: 7.7475\n",
      "Epoch [10/50] Batch 200/1000                   Loss D: 0.0004, loss G: 7.8038\n",
      "Epoch [10/50] Batch 300/1000                   Loss D: 0.0004, loss G: 7.8531\n",
      "Epoch [10/50] Batch 400/1000                   Loss D: 0.0004, loss G: 7.8994\n",
      "Epoch [10/50] Batch 500/1000                   Loss D: 0.0004, loss G: 7.9537\n",
      "Epoch [10/50] Batch 600/1000                   Loss D: 0.0003, loss G: 8.0024\n",
      "Epoch [10/50] Batch 700/1000                   Loss D: 0.0003, loss G: 8.0561\n",
      "Epoch [10/50] Batch 800/1000                   Loss D: 0.0003, loss G: 8.1155\n",
      "Epoch [10/50] Batch 900/1000                   Loss D: 0.0003, loss G: 8.1512\n",
      "Epoch [11/50] Batch 0/1000                   Loss D: 0.0003, loss G: 8.1934\n",
      "Epoch [11/50] Batch 100/1000                   Loss D: 0.0003, loss G: 8.2270\n",
      "Epoch [11/50] Batch 200/1000                   Loss D: 0.0002, loss G: 8.2866\n",
      "Epoch [11/50] Batch 300/1000                   Loss D: 0.0002, loss G: 8.3440\n",
      "Epoch [11/50] Batch 400/1000                   Loss D: 0.0002, loss G: 8.4087\n",
      "Epoch [11/50] Batch 500/1000                   Loss D: 0.0002, loss G: 8.4540\n",
      "Epoch [11/50] Batch 600/1000                   Loss D: 0.0002, loss G: 8.4990\n",
      "Epoch [11/50] Batch 700/1000                   Loss D: 0.0002, loss G: 8.5592\n",
      "Epoch [11/50] Batch 800/1000                   Loss D: 0.0002, loss G: 8.5963\n",
      "Epoch [11/50] Batch 900/1000                   Loss D: 0.0002, loss G: 8.6527\n",
      "Epoch [12/50] Batch 0/1000                   Loss D: 0.0002, loss G: 8.6969\n",
      "Epoch [12/50] Batch 100/1000                   Loss D: 0.0002, loss G: 8.7584\n",
      "Epoch [12/50] Batch 200/1000                   Loss D: 0.0001, loss G: 8.8152\n",
      "Epoch [12/50] Batch 300/1000                   Loss D: 0.0001, loss G: 8.8614\n",
      "Epoch [12/50] Batch 400/1000                   Loss D: 0.0001, loss G: 8.9089\n",
      "Epoch [12/50] Batch 500/1000                   Loss D: 0.0001, loss G: 8.9596\n",
      "Epoch [12/50] Batch 600/1000                   Loss D: 0.0001, loss G: 9.0213\n",
      "Epoch [12/50] Batch 700/1000                   Loss D: 0.0001, loss G: 9.0703\n",
      "Epoch [12/50] Batch 800/1000                   Loss D: 0.0001, loss G: 9.1419\n",
      "Epoch [12/50] Batch 900/1000                   Loss D: 0.0001, loss G: 9.1973\n",
      "Epoch [13/50] Batch 0/1000                   Loss D: 0.0001, loss G: 9.2394\n",
      "Epoch [13/50] Batch 100/1000                   Loss D: 0.0001, loss G: 9.2886\n",
      "Epoch [13/50] Batch 200/1000                   Loss D: 0.0001, loss G: 9.3272\n",
      "Epoch [13/50] Batch 300/1000                   Loss D: 0.0001, loss G: 9.3874\n",
      "Epoch [13/50] Batch 400/1000                   Loss D: 0.0001, loss G: 9.4365\n",
      "Epoch [13/50] Batch 500/1000                   Loss D: 0.0001, loss G: 9.4965\n",
      "Epoch [13/50] Batch 600/1000                   Loss D: 0.0001, loss G: 9.5635\n",
      "Epoch [13/50] Batch 700/1000                   Loss D: 0.0001, loss G: 9.6284\n",
      "Epoch [13/50] Batch 800/1000                   Loss D: 0.0001, loss G: 9.6846\n",
      "Epoch [13/50] Batch 900/1000                   Loss D: 0.0001, loss G: 9.7429\n",
      "Epoch [14/50] Batch 0/1000                   Loss D: 0.0001, loss G: 9.7935\n",
      "Epoch [14/50] Batch 100/1000                   Loss D: 0.0001, loss G: 9.8473\n",
      "Epoch [14/50] Batch 200/1000                   Loss D: 0.0000, loss G: 9.9015\n",
      "Epoch [14/50] Batch 300/1000                   Loss D: 0.0000, loss G: 9.9515\n",
      "Epoch [14/50] Batch 400/1000                   Loss D: 0.0000, loss G: 10.0019\n",
      "Epoch [14/50] Batch 500/1000                   Loss D: 0.0000, loss G: 10.0653\n",
      "Epoch [14/50] Batch 600/1000                   Loss D: 0.0000, loss G: 10.1203\n",
      "Epoch [14/50] Batch 700/1000                   Loss D: 0.0000, loss G: 10.1738\n",
      "Epoch [14/50] Batch 800/1000                   Loss D: 0.0000, loss G: 10.2339\n",
      "Epoch [14/50] Batch 900/1000                   Loss D: 0.0000, loss G: 10.2794\n",
      "Epoch [15/50] Batch 0/1000                   Loss D: 0.0000, loss G: 10.3336\n",
      "Epoch [15/50] Batch 100/1000                   Loss D: 0.0000, loss G: 10.3833\n",
      "Epoch [15/50] Batch 200/1000                   Loss D: 0.0000, loss G: 10.4325\n",
      "Epoch [15/50] Batch 300/1000                   Loss D: 0.0000, loss G: 10.4902\n",
      "Epoch [15/50] Batch 400/1000                   Loss D: 0.0000, loss G: 10.5501\n",
      "Epoch [15/50] Batch 500/1000                   Loss D: 0.0000, loss G: 10.6113\n",
      "Epoch [15/50] Batch 600/1000                   Loss D: 0.0000, loss G: 10.6591\n",
      "Epoch [15/50] Batch 700/1000                   Loss D: 0.0000, loss G: 10.7009\n",
      "Epoch [15/50] Batch 800/1000                   Loss D: 0.0000, loss G: 10.7596\n",
      "Epoch [15/50] Batch 900/1000                   Loss D: 0.0000, loss G: 10.7999\n",
      "Epoch [16/50] Batch 0/1000                   Loss D: 0.0000, loss G: 10.8604\n",
      "Epoch [16/50] Batch 100/1000                   Loss D: 0.0000, loss G: 10.9227\n",
      "Epoch [16/50] Batch 200/1000                   Loss D: 0.0000, loss G: 10.9909\n",
      "Epoch [16/50] Batch 300/1000                   Loss D: 0.0000, loss G: 11.0365\n",
      "Epoch [16/50] Batch 400/1000                   Loss D: 0.0000, loss G: 11.1025\n",
      "Epoch [16/50] Batch 500/1000                   Loss D: 0.0000, loss G: 11.1523\n",
      "Epoch [16/50] Batch 600/1000                   Loss D: 0.0000, loss G: 11.2208\n",
      "Epoch [16/50] Batch 700/1000                   Loss D: 0.0000, loss G: 11.2704\n",
      "Epoch [16/50] Batch 800/1000                   Loss D: 0.0000, loss G: 11.3283\n",
      "Epoch [16/50] Batch 900/1000                   Loss D: 0.0000, loss G: 11.3848\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real.float()).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bayesianNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c0cf972fe55eaf0c962c4929f592d86a72c532b00283f932a90435beee88e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
