{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to override __init__, __len__, __getitem__\n",
    "# # as per datasets requirement\n",
    "# class PathsDataset(torch.utils.data.Dataset):\n",
    "#     # init the dataset, shape = L x W\n",
    "#     def __init__(self, path, transform=None, shape = (100,100)):\n",
    "#         print(\"Loading paths dataset...\")\n",
    "#         # Read in path files\n",
    "#         # Convert to x by y np arrays\n",
    "#         # add the np arrays to a list\n",
    "#         # set self.transform and self.data\n",
    "#         self.paths = [] # create a list to hold all paths read from file\n",
    "#         for filename in os.listdir(path):\n",
    "#             with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "#                 self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "#                 self.path = np.asarray(self.flat_path).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "                \n",
    "#                 # xvales which to interpolate on\n",
    "#                 # want to interpolate on xvalues from the min xval in the path to the largest xval in the path\n",
    "#                 self.xvals = np.linspace(int(min(self.path[:,0])), int(max(self.path[:,0])), int(max(self.path[:,0])-min(self.path[:,0])))\n",
    "#                 self.xvals = self.xvals.astype(int)\n",
    "\n",
    "#                 # interpolate for all xvals using the paths from file's x and y values\n",
    "#                 self.interp_path = np.interp(self.xvals, self.path[:,0], self.path[:,1])\n",
    "#                 self.interp_path = np.array(self.interp_path).astype(int)\n",
    "\n",
    "#                 # create a LxW matrix where all the values where path is equal to 1\n",
    "#                 self.path_matrix = np.zeros(shape)\n",
    "#                 self.path_matrix[self.interp_path, self.xvals] = 1\n",
    "                \n",
    "\n",
    "#                 self.paths.append(self.path_matrix) # add the path to paths list\n",
    "#         self.transform = transform\n",
    "#         print(\"Done!\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # print(\"getitem\")\n",
    "#         # if torch.is_tensor(idx):\n",
    "#         #     idx = idx.tolist()\n",
    "#         # imagePath = self.paths_file + \"/\" + self.data['Image_path'][idx]\n",
    "#         # image = sk.imread(imagePath)\n",
    "#         # label = self.data['Condition'][idx]\n",
    "#         # image = Image.fromarray(image)\n",
    "\n",
    "#         # if self.sourceTransform:\n",
    "#         #     image = self.sourceTransform(image)\n",
    "#         x = np.float32(self.paths[idx])\n",
    "\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "            \n",
    "\n",
    "#         return x\n",
    "\n",
    "#         #return image, label\n",
    "\n",
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path, transform=None, shape = (100,100)):\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x).cuda()\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 5e-4\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 20\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "#Speicific to WGAN\n",
    "WEIGHT_CLIP = 0.01 # C param from WGAN paper\n",
    "CRITIC_ITERATIONS = 5 # how many times the critic loop runs for each generator loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToPILImage(),\n",
    "        # tfms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "\n",
    "dataset = PathsDataset(path = \"./data/map_64x64/\", shape = (64,64), transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,\n",
    "#                        download=True)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
    "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/313                   Loss D: -0.7270, loss G: 0.3945\n",
      "Epoch [0/20] Batch 100/313                   Loss D: -0.5420, loss G: 0.6440\n",
      "Epoch [0/20] Batch 200/313                   Loss D: -1.4832, loss G: 0.6810\n",
      "Epoch [0/20] Batch 300/313                   Loss D: -1.5181, loss G: 0.7213\n",
      "Epoch [1/20] Batch 0/313                   Loss D: -1.5223, loss G: 0.7214\n",
      "Epoch [1/20] Batch 100/313                   Loss D: -1.5105, loss G: 0.7156\n",
      "Epoch [1/20] Batch 200/313                   Loss D: -1.5093, loss G: 0.7143\n",
      "Epoch [1/20] Batch 300/313                   Loss D: -1.5220, loss G: 0.7185\n",
      "Epoch [2/20] Batch 0/313                   Loss D: -1.5327, loss G: 0.7199\n",
      "Epoch [2/20] Batch 100/313                   Loss D: -1.4937, loss G: 0.7120\n",
      "Epoch [2/20] Batch 200/313                   Loss D: -1.4900, loss G: 0.7116\n",
      "Epoch [2/20] Batch 300/313                   Loss D: -1.3570, loss G: 0.6801\n",
      "Epoch [3/20] Batch 0/313                   Loss D: -1.4690, loss G: 0.6992\n",
      "Epoch [3/20] Batch 100/313                   Loss D: -1.4811, loss G: 0.7016\n",
      "Epoch [3/20] Batch 200/313                   Loss D: -1.4580, loss G: 0.6940\n",
      "Epoch [3/20] Batch 300/313                   Loss D: -1.3637, loss G: 0.6451\n",
      "Epoch [4/20] Batch 0/313                   Loss D: -1.4566, loss G: 0.6961\n",
      "Epoch [4/20] Batch 100/313                   Loss D: -1.4455, loss G: 0.6838\n",
      "Epoch [4/20] Batch 200/313                   Loss D: -0.0757, loss G: 0.6938\n",
      "Epoch [4/20] Batch 300/313                   Loss D: -1.4101, loss G: 0.6766\n",
      "Epoch [5/20] Batch 0/313                   Loss D: -0.2392, loss G: 0.5196\n",
      "Epoch [5/20] Batch 100/313                   Loss D: -1.3241, loss G: 0.6319\n",
      "Epoch [5/20] Batch 200/313                   Loss D: -1.1374, loss G: 0.2488\n",
      "Epoch [5/20] Batch 300/313                   Loss D: -1.2754, loss G: 0.6576\n",
      "Epoch [6/20] Batch 0/313                   Loss D: -1.1829, loss G: 0.5336\n",
      "Epoch [6/20] Batch 100/313                   Loss D: -1.1431, loss G: 0.5635\n",
      "Epoch [6/20] Batch 200/313                   Loss D: -1.1367, loss G: 0.5269\n",
      "Epoch [6/20] Batch 300/313                   Loss D: -1.1523, loss G: 0.4861\n",
      "Epoch [7/20] Batch 0/313                   Loss D: -1.1849, loss G: 0.6216\n",
      "Epoch [7/20] Batch 100/313                   Loss D: -0.9855, loss G: 0.3501\n",
      "Epoch [7/20] Batch 200/313                   Loss D: -0.9696, loss G: 0.4092\n",
      "Epoch [7/20] Batch 300/313                   Loss D: -0.9661, loss G: 0.0992\n",
      "Epoch [8/20] Batch 0/313                   Loss D: -0.7339, loss G: -0.0312\n",
      "Epoch [8/20] Batch 100/313                   Loss D: -0.5462, loss G: 0.4724\n",
      "Epoch [8/20] Batch 200/313                   Loss D: -0.9807, loss G: 0.3699\n",
      "Epoch [8/20] Batch 300/313                   Loss D: -0.9286, loss G: 0.3854\n",
      "Epoch [9/20] Batch 0/313                   Loss D: -1.0164, loss G: 0.6160\n",
      "Epoch [9/20] Batch 100/313                   Loss D: -0.9397, loss G: 0.4228\n",
      "Epoch [9/20] Batch 200/313                   Loss D: -0.8524, loss G: 0.0138\n",
      "Epoch [9/20] Batch 300/313                   Loss D: -0.9253, loss G: 0.1772\n",
      "Epoch [10/20] Batch 0/313                   Loss D: -1.0580, loss G: 0.4624\n",
      "Epoch [10/20] Batch 100/313                   Loss D: -0.8035, loss G: 0.1690\n",
      "Epoch [10/20] Batch 200/313                   Loss D: -0.8527, loss G: 0.1580\n",
      "Epoch [10/20] Batch 300/313                   Loss D: -0.3930, loss G: 0.3827\n",
      "Epoch [11/20] Batch 0/313                   Loss D: -0.7319, loss G: 0.2305\n",
      "Epoch [11/20] Batch 100/313                   Loss D: -0.7763, loss G: 0.4238\n",
      "Epoch [11/20] Batch 200/313                   Loss D: -0.4245, loss G: 0.3153\n",
      "Epoch [11/20] Batch 300/313                   Loss D: -0.8322, loss G: 0.2890\n",
      "Epoch [12/20] Batch 0/313                   Loss D: -0.7733, loss G: 0.2597\n",
      "Epoch [12/20] Batch 100/313                   Loss D: -0.8227, loss G: 0.5978\n",
      "Epoch [12/20] Batch 200/313                   Loss D: -0.8031, loss G: 0.4433\n",
      "Epoch [12/20] Batch 300/313                   Loss D: -0.8832, loss G: 0.0335\n",
      "Epoch [13/20] Batch 0/313                   Loss D: -0.4997, loss G: 0.4315\n",
      "Epoch [13/20] Batch 100/313                   Loss D: -0.5484, loss G: 0.5135\n",
      "Epoch [13/20] Batch 200/313                   Loss D: -0.7257, loss G: -0.1079\n",
      "Epoch [13/20] Batch 300/313                   Loss D: -0.8504, loss G: 0.1219\n",
      "Epoch [14/20] Batch 0/313                   Loss D: -0.7718, loss G: 0.4518\n",
      "Epoch [14/20] Batch 100/313                   Loss D: -0.4534, loss G: 0.4063\n",
      "Epoch [14/20] Batch 200/313                   Loss D: -0.7817, loss G: 0.4776\n",
      "Epoch [14/20] Batch 300/313                   Loss D: -0.8377, loss G: 0.4575\n",
      "Epoch [15/20] Batch 0/313                   Loss D: -0.5695, loss G: -0.0508\n",
      "Epoch [15/20] Batch 100/313                   Loss D: -0.7097, loss G: 0.3843\n",
      "Epoch [15/20] Batch 200/313                   Loss D: -0.7534, loss G: 0.1839\n",
      "Epoch [15/20] Batch 300/313                   Loss D: -0.7444, loss G: 0.4342\n",
      "Epoch [16/20] Batch 0/313                   Loss D: -0.7560, loss G: 0.1998\n",
      "Epoch [16/20] Batch 100/313                   Loss D: -0.6887, loss G: 0.2278\n",
      "Epoch [16/20] Batch 200/313                   Loss D: -0.5913, loss G: -0.0744\n",
      "Epoch [16/20] Batch 300/313                   Loss D: -0.8219, loss G: 0.2596\n",
      "Epoch [17/20] Batch 0/313                   Loss D: -0.5429, loss G: 0.5135\n",
      "Epoch [17/20] Batch 100/313                   Loss D: -0.7213, loss G: 0.2454\n",
      "Epoch [17/20] Batch 200/313                   Loss D: -0.3889, loss G: 0.3679\n",
      "Epoch [17/20] Batch 300/313                   Loss D: -0.6890, loss G: 0.5481\n",
      "Epoch [18/20] Batch 0/313                   Loss D: -0.6144, loss G: 0.5655\n",
      "Epoch [18/20] Batch 100/313                   Loss D: -0.7020, loss G: 0.1913\n",
      "Epoch [18/20] Batch 200/313                   Loss D: -0.5931, loss G: -0.2911\n",
      "Epoch [18/20] Batch 300/313                   Loss D: -0.7357, loss G: 0.5996\n",
      "Epoch [19/20] Batch 0/313                   Loss D: -0.7403, loss G: 0.3822\n",
      "Epoch [19/20] Batch 100/313                   Loss D: -0.5504, loss G: -0.2051\n",
      "Epoch [19/20] Batch 200/313                   Loss D: -0.7502, loss G: 0.3799\n",
      "Epoch [19/20] Batch 300/313                   Loss D: -0.6045, loss G: 0.3606\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) #   want to maximize (according to paper) but \n",
    "                                                                            #   optim algorithms are for minimizing so take - \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # want to re use the computations for fake for generator\n",
    "            opt_critic.step()\n",
    "\n",
    "            for p in critic.parameters():\n",
    "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "fake = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcd102f1bd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU2UlEQVR4nO3dfYxcV3nH8e9vxl6vX/JmkphNHHAQ4SUg4oDrOISikJAXQoSLSqpEpTKtWwuJoqAikaRVK0CqSFUVgaqqlQsUI1IgIqSJQsiLDFFLIU4c4kBMyEsTJ3Hs2MFpsJPUL7vz9I+5O3PvsLM73p25M7vn95GsOffembmPZ+eZc+69556jiMDM5r5KvwMws3I42c0S4WQ3S4ST3SwRTnazRDjZzRIxo2SXdKmkRyU9IenabgVlZt2n6V5nl1QFHgMuAnYC9wNXRcQvuxeemXXLvBm8djXwREQ8CSDp28BaoG2yD1UWxsLqMQDE6Gjne1Kb1ZVqYTnGxprbhuYXtx0+knth7g173KlosjhUKTasolZrblsw1NyQjx0K8edfM1Dyf7PcRywV/5j5ykbVSf6e85tf1TjS8t1ps69sh/mdNVe3fPaF70G15e+Si6P1/QvvM68Zf2uMyr1njI4Vt+Vfl9/W+r3Pf46t350s/oPxCofj4IQZM5NkPxV4Nre8EzhnshcsrB7DuUs/AsDYCy8UN06SgJo3cZiVJYsLy2P7X26U5712pLBtdOdzzfcbaiZSHDo0WcidUZtfI2DeKcuLcTyzs1GuLFpU2FZ79dXm65a/vrl+1/PFN53f/AGpHThwVKF2VYd/s/wPe2V4uPC8Wu7zrx53fGHb2G/2N8rzTjqpUR59fk9xX7kfiahF+22jzR/NypIlhedxJPcjvLjl75KLo/X9Kwub/5/KiUubsT+/t/i8Y5r7G9v3YmFb9YSlE277rR+//Oe4sBjj+A/SvYd+QDszSfaJvuG/VU1K2gBsABiuLPmtF5hZOWZyzH4u8NmIuCRbvg4gIr7Q7jXHammcU724vlAba/c0S8i81y5rlFtr7LZaDt8G5ruUj2uymHoRf/aeW8buYn+8OGFTcyZn4+8HzpB0uqQh4Erg1hm8n5n10LSb8RExKunPgTuBKvC1iNjetcjMrKtmcsxORNwO3N6lWMysh2aU7EdNoEr9cCIG9IqRlavj4/S8QTlGb9VhXOM5MC6iC5eCO0god5c1S4ST3SwR5Tbj4yh7zpnNQa0dc3rdi3Oca3azRDjZzRLhZDdLRLnH7NDsKjiol0/Meq1P333X7GaJcLKbJaL8ZryZzZgWLCgsV44/rr7+1+1T2jW7WSKc7GaJKPlGGDWG2gmfjTc7OrlBL1qHUxt7YV99/Vj7Hqqu2c0S4WQ3S4ST3SwRpR6zS2qMox5HDpe5a7NZb96K0xrl0Sd3FLZVsrzSxEPG15/Tk6jMbOA42c0SUWozPqLWnRlYzBLU2nTPqx08CBSn02rlmt0sEU52s0Q42c0SUf6Ak62D7ZnZzI13pZ1sirmp3kPS1yTtlfRwbt1SSXdLejx7PGHm0ZpZL3XSjP86cGnLumuBzRFxBrA5WzazATZlMz4i/lPSipbVa4Hzs/Im4B7gmin3NiDTP93+3M8a5ctOfWf/AjHrlh5O/7QsInYDZI8nT/N9zKwkPT9BJ2kDsAFgmEW93p2ZtTHdmn2PpBGA7HFvuydGxMaIWBURq+azgBgd7fsUUFVVGv/M5gJVq/WBYdrfBzPtZL8VWJeV1wG3TPN9zKwknVx6+xbwU+DNknZKWg9cD1wk6XHgomzZzAZYJ2fjr2qz6cIux2JmPVT+gJPZeNf9vPvtklNWNsp37trWdpvZbBFjWde5STqo+gyVWSKc7GaJSH76p9Zme75ZP9bSK6ldb7vf/OGawvK9f/8v7fe3/F2NcnXJ4sK2sf37JwvVrK3KonofFr3avv52zW6WCCe7WSKc7GaJSP6YvdVkl97yd8ut/tmVjXJ8v9hHcfLLd83RBWbbMbrmFb8u/e72PJH83wjSuaux9uqrAESt/d1vrtnNEuFkN0uEJhtnutuO1dI4p3pxfcFTNs86N+28t7D8+8vXtHlmb31g+0uF5R+87fi+xFG63JTNrfmj+UMA3HvkDvbX9k1475trdrNEONnNElH+jTDVelMk3IyfdVqb7fnehmXeQNTabO9XHKWbJGcasyJ7+iczc7KbJcLJbpaIUo/ZJaGh+UDuGMNmrff9yZ81yj/a9a+FbR/dcX6j/MK7X2qURy98V+F5d3xjY6M8X1U60Xpcnl8evwQ1rt33bOd17y4sL//CTzra98BSdrXNg1eYmZPdLBGl9qA7bt6Jce6StcDsuwnEjk7lmGMa5R88+l+N8n8fLN6o8fk3HP2NKq3jBub14tLb+LiJ0N+xEzuxJTazP150DzqzlDnZzRLhZDdLRMmDVwgqk0xGZXNG7cCBRrnbx9Gt75cfsKIyPFyM4+DBGe9v0I/TO9XJ9E+nSfqRpEckbZd0dbZ+qaS7JT2ePZ7Q+3DNbLo6acaPAp+OiLcCa4BPSDoTuBbYHBFnAJuzZTMbUJ3M9bYb2J2VD0h6BDgVWAucnz1tE3APcM0Ub0YcPjL9aM0msHfs1UZZQ8UedHShGT9XHNUJOkkrgLOBLcCy7Idg/Afh5K5HZ2Zd03GyS1oC3AR8KiI67hEjaYOkrZK2Hg7/ypr1S0fJLmk+9US/ISK+l63eI2kk2z4C7J3otRGxMSJWRcSqIQ1P9BQzK8GUx+ySBHwVeCQivpjbdCuwDrg+e7yloz3Kl96suz72uvc0yrc/d09hWyrjxneik+vs5wF/BPxC0rZs3V9ST/IbJa0HngGu6EmEZtYVnZyN/zHQrjq+sLvhmFmvlNqDLnzpzXqsKvcAb8efjFkinOxmiSj3RpgIYtTNeBtAk0ytNFe4ZjdLhJPdLBFOdrNElDx4hdmAmqPH6Xmu2c0S4WQ3S0T5zfjxHk4x95tNNsBab8gqcf6EfnHNbpYIJ7tZIpzsZokod8rmSoXK4kVAcVxxs9IlcIzeyjW7WSKc7GaJKHfwilqN2iuvTv1Es2kai+KU0IVLbAk23fNcs5slwslulohyz8ZXK1SXLAZgbH/H80yYdax16Og7dz3YKHd7NtnZxjW7WSKc7GaJcLKbJaLcS29jNcZefqXMXZpZZsqaXdKwpPskPSRpu6TPZeuXSrpb0uPZ4wm9D9fMpquTZvwh4IKIOAtYCVwqaQ1wLbA5Is4ANmfLZjagOpnrLYCXs8X52b8A1gLnZ+s3AfcA10z2XnP10tuah4pj4d971vw+RWKtvrH/xH6HMDA6nZ+9ms3guhe4OyK2AMsiYjdA9nhyz6I0sxnrKNkjYiwiVgLLgdWS3t7pDiRtkLRV0tbDtYPTDNPMZuqoLr1FxEvUm+uXAnskjQBkj3vbvGZjRKyKiFVDleGZRWtm0zblMbukk4AjEfGSpIXA+4G/A24F1gHXZ4+3TPVeUQtq/zf3avfbnnlbYflEHutTJNbqhrcsb5Tv3LWtUU6x62wn19lHgE2SqtRbAjdGxG2SfgrcKGk98AxwRQ/jNLMZ6uRs/M+BsydYvw+4sBdBmVn3lT9ufEVTP2cWuHnnfY3yh5dP8kSzAeG+8WaJcLKbJaL8Znxt9o4Dpnc1z7q76T775M/A58/Mt26bq1yzmyXCyW6WCCe7WSIG59Jb6xS6eQMy3nc8sL3fIViXXPCxPy0sX/vEpkb5H974ttanzwmu2c0S4WQ3S0S5zfgI4tChtts6Uqk2y7WxmcdkSZp/19bC8sWLmgOQfHFeMS1idLSUmHrNNbtZIpzsZolwspslovxLb+OX2KZ7Oc3H6dYD+e6ytz23pbDt8uWrmgsDchl4OlyzmyXCyW6WiPKb8WYDKD/2/9r3X1nYVl26r1Ee2/diaTF1m2t2s0Q42c0SUW4zXqBqvQfcXOmVZHPD505q3uR0ySNzc/ou1+xmiXCymyXCyW6WiFKP2aUKWrAA8DG79Vfl7W9pWbOtH2GUquOaPZu2+UFJt2XLSyXdLenx7PGE3oVpZjN1NM34q4FHcsvXApsj4gxgc7ZsZgOqo2a8pOXAB4G/Bf4iW70WOD8rb6I+lfM1k71PRLj5bgPhpju+UVi+ZPm5uaW5ebNVpzX7l4DPALXcumURsRsgezy5u6GZWTdNmeySLgf2RsQD09mBpA2StkraeiTm3tzsZrNFJ83484APSboMGAaOlfRNYI+kkYjYLWkE2DvRiyNiI7AR4NjKa2bvzcBms9yUNXtEXBcRyyNiBXAl8MOI+ChwK7Aue9o64JaeRWnWZYsqQ4V/1Maa/+aomXSquR64SNLjwEXZspkNqKPqVBMR91A/605E7AMu7H5IZtYLnrLZknT67cXpn95UebC5MEeb8u4bb5YIJ7tZIsqf/mn0yNTPM+uxpy77SmH5ktrK/gRSItfsZolwspslwsluloiSB5wUGhoCaD91s1kJTr91Q2H5zBXPN8qjO54pO5xSuGY3S4ST3SwR5Y5BV6lQWTgMwJib8dZHb/r4fYXlbz77k0b5ytPeXXY4pXDNbpYIJ7tZIpzsZokov7vsYXeXtf579q+Lx+UnVLf1J5ASuWY3S4ST3SwRpTbjI4I44nHjrT/mnXpKo/zgx79c2PbBcz6cW9pZUkTlcs1ulggnu1kiyh+DzqxPvn//7Y3yJaf8TsvWudl0z3PNbpYIJ7tZIpzsZoko9643CQ0vACCOHC5z15aoO3dta5QvOfXsRvkfn/5x4XmffP15ZYXUN53Oz74DOEB94urRiFglaSnwHWAFsAP4g4j4396EaWYzdTTN+PdFxMqIWJUtXwtsjogzgM3ZspkNqJk049cC52flTdTngLtmshdEBHHYzXfrnZt3Fgel+ODqZs+4rz79nUb5+j2t0xTO/cFUOq3ZA7hL0gOSxkfqWxYRuwGyx5N7EaCZdUenNft5EbFL0snA3ZJ+1ekOsh+HDQDDLJpGiGbWDR3V7BGxK3vcC9wMrAb2SBoByB73tnntxohYFRGr5mu4O1Gb2VGbsmaXtBioRMSBrHwx8HngVmAdcH32eMuUe/Ndb9YD80Ze2yh/5IKrCts2/mRTo7z+de9pblDx3FHhEt0pK7sa36DopBm/DLhZ0vjz/z0i7pB0P3CjpPXAM8AVvQvTzGZqymSPiCeBsyZYvw9oPaVpZgOq/OmfqlUAojZW6q5tFqpUm+Xc9+WZzxbHj1t2f3Ncw12/W/xKF5rueRGFxXzTPd+kbzWbm/juG2+WCCe7WSKc7GaJULQcu/TScQtH4tw3rgdgbPujpe3XZj8tWNAot073rflDzW09vpuy9Xh+0I7ht8Rm9seLmmiba3azRDjZzRJR7rjxhw5Te+ypMndpc0Rr072wrcSBUFqb7e0u0w1a8x5cs5slw8lulojyZ3Edy3pCSb+1zWy2addcH8Sz9q7ZzRLhZDdLhJPdLBH9u+vtiO96s7lrEI7RW7lmN0uEk90sEeVO/1QRlYX1QSfHPP1TOta8o1Gct+c3hU2jTz3dKB+8fHVh2/BtxTHgG/KDWkBhYAtrzzW7WSKc7GaJcLKbJaLcS2/z58NINkvUgQPFbZN1l80fo0Wts9fY4Lj3541i66wBleHmxCGtx+iVxYsb5dorrzQ3+Bh9WlyzmyXCyW6WiHKb8WM19PKr9bJafmdikqaZm21zVu3gwfbb8k13m7GOanZJx0v6rqRfSXpE0rmSlkq6W9Lj2eMJvQ7WzKav02b8l4E7IuIt1KeCegS4FtgcEWcAm7NlMxtQUya7pGOB9wJfBYiIwxHxErAWGJ8icxPwe1PurTZGHHiZOPByvWme/2dmPdVJzf4G4AXg3yQ9KOkr2dTNyyJiN0D2eHIP4zSzGeok2ecB7wT+OSLOBl7hKJrskjZI2ipp6+Fa+5MxZtZbnST7TmBnRGzJlr9LPfn3SBoByB73TvTiiNgYEasiYtVQZXiip5hZCaZM9oh4HnhW0puzVRcCvwRuBdZl69YBt0y5t/EedCNu8ZuVrdPr7J8EbpA0BDwJ/DH1H4obJa0HngGu6E2IZtYNHSV7RGwDVk2w6cKuRmNmPVNuD7rDR4hde4DirJww+fQ+ZpPKz0Hgm6Pact94s0Q42c0S4WQ3S0S5x+zVCpUl2YAElWMKm2ovNQciHB9bflyMNoc8aMwVBzDW0s02/7pa8dit7bS+rXPOTcbHg+Vq87epHndsYbn2xtMa5dj6cE9DGljjn9VkY8CUE4mZ9ZuT3SwRihKbppJeAJ4GTgR+XdqO23McRY6jaBDiONoYXh8RJ020odRkb+xU2hoRE3XScRyOw3H0KAY3480S4WQ3S0S/kn1jn/bbynEUOY6iQYijazH05ZjdzMrnZrxZIkpNdkmXSnpU0hOSShuNVtLXJO2V9HBuXelDYUs6TdKPsuG4t0u6uh+xSBqWdJ+kh7I4PtePOHLxVLPxDW/rVxySdkj6haRtkrb2MY6eDdteWrJLqgL/BHwAOBO4StKZJe3+68ClLev6MRT2KPDpiHgrsAb4RPYZlB3LIeCCiDgLWAlcKmlNH+IYdzX14cnH9SuO90XEytylrn7E0bth2yOilH/AucCdueXrgOtK3P8K4OHc8qPASFYeAR4tK5ZcDLcAF/UzFmAR8DPgnH7EASzPvsAXALf1628D7ABObFlXahzAscBTZOfSuh1Hmc34U4Fnc8s7s3X90tehsCWtAM4GtvQjlqzpvI36QKF3R31A0X58Jl8CPgPkpuftSxwB3CXpAUkb+hRHT4dtLzPZJ7qFKclLAZKWADcBn4qI/f2IISLGImIl9Zp1taS3lx2DpMuBvRHxQNn7nsB5EfFO6oeZn5D03j7EMKNh26dSZrLvBE7LLS8HdpW4/1YdDYXdbZLmU0/0GyLie/2MBSDqs/vcQ/2cRtlxnAd8SNIO4NvABZK+2Yc4iIhd2eNe4GZgdR/imNGw7VMpM9nvB86QdHo2Su2V1Iej7pejHwp7hiSJ+jRaj0TEF/sVi6STJB2flRcC7wd+VXYcEXFdRCyPiBXUvw8/jIiPlh2HpMWSjhkvAxcDD5cdR3Rz2PY2OyjtH3AZ8BjwP8BflbjfbwG7gSPUfz3XA6+hfmLo8exxaQlxvIf6ocvPgW3Zv8vKjgV4B/BgFsfDwN9k60v/THIxnU/zBF3Zn8cbgIeyf9vHv5t9+o6sBLZmf5v/AE7oVhzuQWeWCPegM0uEk90sEU52s0Q42c0S4WQ3S4ST3SwRTnazRDjZzRLx/3oeOZgEVEQgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(fake.cpu().detach().numpy()[7][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bayesianNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c0cf972fe55eaf0c962c4929f592d86a72c532b00283f932a90435beee88e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
