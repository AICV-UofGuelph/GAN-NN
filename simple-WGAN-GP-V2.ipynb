{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os, math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, features, device='cpu'):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, features, kernel_size=4, stride=2, padding=1, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features * 2, kernel_size=4, stride=2, padding=1, bias=False, device=device),\n",
    "            nn.InstanceNorm2d(features * 2, affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(features * 2, features * 4, kernel_size=4, stride=2, padding=1, bias=False, device=device),\n",
    "            nn.InstanceNorm2d(features * 4, affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(features * 4, features * 8, kernel_size=4, stride=2, padding=1, bias=False, device=device),\n",
    "            nn.InstanceNorm2d(features * 8, affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, 1, kernel_size=4, stride=2, padding=0, device=device), # convert to single channel\n",
    "            nn.AdaptiveAvgPool2d(1),    # pool the matrix into a single value for sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "        y = self.block5(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input channels (noise, map, and initial path)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, features, device='cpu'):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, features, 3, 1, 2, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, 3, 1, 2, device=device),\n",
    "            nn.InstanceNorm2d(features*2, affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(features*2, features, 3, 1, 2, device=device),\n",
    "            nn.BatchNorm2d(features, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(features, 1, 3, 1, 2, device=device),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "\n",
    "        # y = F.adaptive_max_pool2d(y, output_size=map_shape)\n",
    "\n",
    "        y = self._round(y)\n",
    "        return y\n",
    "    \n",
    "    def _round(self, mat):\n",
    "        # TODO: cite something? (this function is based off of Thor's code)\n",
    "        mat_hard = torch.round(mat)\n",
    "        mat = (mat_hard - mat.data) + mat\n",
    "\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    if real.shape != fake.shape:\n",
    "        print(\"not same shape\")\n",
    "    interpolated_images = real * epsilon + fake * (1-epsilon) #interpolate epsilon % real image, (1-epsilon fake image)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # compute grad of mixed scores w.r.t interpolated image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path_dir, map_file, transform=None, shape = (100,100), device='cpu'):\n",
    "        self.device = device\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        self.map = np.loadtxt(map_file, skiprows=2).reshape(shape)\n",
    "        self.map = self.map[np.newaxis, :, :]\n",
    "        for filename in os.listdir(path_dir):\n",
    "            with open(os.path.join(path_dir, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Add Initial Path onto the loaded path matrix\n",
    "        initial_path = np.zeros_like(path_mat)\n",
    "\n",
    "        # Create Straight line between start/end points\n",
    "        x1 = path[0,0]\n",
    "        y1 = path[0,1]\n",
    "        x2 = path[path.shape[0]-1,0]\n",
    "        y2 = path[path.shape[0]-1,1]\n",
    "\n",
    "        # x = x1\n",
    "        # y = y1\n",
    "\n",
    "        # if (x1 < x2):\n",
    "        #     x_dir = 1\n",
    "        # else:\n",
    "        #     x_dir = -1\n",
    "\n",
    "        # if (y1 < y2):\n",
    "        #     y_dir = 1\n",
    "        # else:\n",
    "        #     y_dir = -1\n",
    "\n",
    "        # # Determine y from x\n",
    "        # if x2-x1 != 0:\n",
    "        #     m = (y2-y1)/(x2-x1)\n",
    "        #     while x != x2:\n",
    "        #         y = round(m*(x-x1) + y1)\n",
    "        #         initial_path[y,x] = 1\n",
    "        #         x += x_dir\n",
    "        # else:\n",
    "        #     while x != x2:\n",
    "        #         initial_path[y1,x] = 1\n",
    "        #         x += x_dir\n",
    "\n",
    "        # x = x1\n",
    "        # y = y1\n",
    "        # # Determine x from y\n",
    "        # if y2-y1 != 0:\n",
    "        #     m = (x2-x1)/(y2-y1)\n",
    "        #     while y != y2:\n",
    "        #         x = round(m*(y-y1) + x1)\n",
    "        #         initial_path[y,x] = 1\n",
    "        #         y += y_dir\n",
    "        # else:\n",
    "        #     while y != y2:\n",
    "        #         initial_path[y,x1] = 1\n",
    "        #         y += y_dir\n",
    "\n",
    "        initial_path[y1,x1] = 1     # Include the first point in the path\n",
    "        initial_path[y2,x2] = 1     # Include the last point in the path\n",
    "\n",
    "        slope = -0.05\n",
    "\n",
    "        for x in range(0, len(initial_path)):\n",
    "            for y in range(0, len(initial_path[x])):\n",
    "                dis_start = math.sqrt((x-x1)**2 + (y-y1)**2)\n",
    "                dis_goal = math.sqrt((x-x2)**2 + (y-y2)**2)\n",
    "                dis = dis_start if dis_start < dis_goal else dis_goal\n",
    "\n",
    "                height = slope*dis + 1\n",
    "\n",
    "                if height < 0:\n",
    "                    initial_path[y][x] = 0\n",
    "                else:\n",
    "                    initial_path[y][x] = height\n",
    "\n",
    "        path_mat = np.stack((path_mat, initial_path))\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "MAP_NAME = 'map_64x64'\n",
    "MAP_SHAPE = (64,64)\n",
    "# MAP_NAME = '8x12_map'\n",
    "# MAP_SHAPE = (163,243)\n",
    "\n",
    "LOAD = False\n",
    "SAVE = True\n",
    "GEN_PATH = './checkpoints/wgan_gp/generator/'\n",
    "DISC_PATH = './checkpoints/wgan_gp/critic/'\n",
    "LOAD_EPOCH = 0  # The epoch checkpoint to load\n",
    "\n",
    "# # Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 50\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NUM_EPOCHS = 2\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "NOISE_SHAPE = (BATCH_SIZE, 1, MAP_SHAPE[0], MAP_SHAPE[1])\n",
    "\n",
    "#Speicific to WGAN\n",
    "CRITIC_ITERATIONS = 5 # how many times the critic loop runs for each generator loop\n",
    "LAMBDA_GP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        # tfms.ToTensor(),\n",
    "        # tfms.Normalize(\n",
    "        #     [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        # ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = np.loadtxt(f\"./data/{MAP_NAME}/{MAP_NAME}.txt\", skiprows=2).reshape(MAP_SHAPE)\n",
    "map = map[np.newaxis,np.newaxis,:,:]\n",
    "map = np.repeat(map, BATCH_SIZE, axis=0)\n",
    "map = torch.Tensor(map).to(device)\n",
    "\n",
    "dataset = PathsDataset(path_dir = f\"./data/{MAP_NAME}/paths_variety/\", map_file = f\"./data/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=transforms, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_dataset = PathsDataset(path_dir = f\"./env/{MAP_NAME}/subsets/training\", map_file = f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=transforms, device=device)\n",
    "# test_dataset = PathsDataset(path_dir = f\"./env/{MAP_NAME}/subsets/testing\", map_file = f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=transforms, device=device)\n",
    "\n",
    "# dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "# dataloader_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch = 0\n",
    "\n",
    "gen = Generator(FEATURES_GEN, device=device)\n",
    "critic = Discriminator(FEATURES_DISC, device=device)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas = (0.0, 0.9))\n",
    "\n",
    "if LOAD:\n",
    "    # Load gen\n",
    "    checkpoint = torch.load(f'{GEN_PATH}epoch-{LOAD_EPOCH}.tar')\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt_gen.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    curr_epoch = checkpoint['epoch']\n",
    "    loss_gen = checkpoint['loss']\n",
    "\n",
    "    # Load critic\n",
    "    checkpoint = torch.load(f'{DISC_PATH}epoch-{LOAD_EPOCH}.tar')\n",
    "    critic.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt_critic.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    curr_epoch = checkpoint['epoch']\n",
    "    loss_critic = checkpoint['loss']\n",
    "else:\n",
    "    initialize_weights(gen)\n",
    "    initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(NOISE_SHAPE, device=device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_overlay = SummaryWriter(f\"logs/fake_overlay\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    curr_epoch += 1\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        real = torch.concat((real, map), axis=1)\n",
    "\n",
    "        initial_path = real[:,1:2,:,:]\n",
    "        noise = torch.randn(NOISE_SHAPE, device=device)\n",
    "        noise = torch.concat((noise, initial_path, map), axis=1)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        fixed_input = torch.concat((fixed_noise, initial_path, map), axis=1)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(NOISE_SHAPE, device=device)\n",
    "            noise = torch.concat((noise, initial_path, map), axis=1)\n",
    "            fake = gen(noise)\n",
    "            fake = torch.concat((fake, initial_path, map), axis=1)\n",
    "            critic_real = critic(real)\n",
    "            critic_fake = critic(fake)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device) # compute the gradient penalty\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP*gp\n",
    "            ) #   want to maximize (according to paper) but \n",
    "                                                                            #   optim algorithms are for minimizing so take - \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # want to re use the computations for fake for generator\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_input)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                fake = torch.concat((fake, initial_path, map), axis=1)\n",
    "                img_grid_fake_overlay = torchvision.utils.make_grid(\n",
    "                    fake[:BATCH_SIZE], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                writer_overlay.add_image(\"Fake\", img_grid_fake_overlay, global_step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    # save generator checkpoint\n",
    "    if SAVE:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': gen.state_dict(),\n",
    "                    'optimizer_state_dict': opt_gen.state_dict(),\n",
    "                    'loss': loss_gen,\n",
    "        }, f\"{GEN_PATH}epoch-{epoch}.tar\")\n",
    "\n",
    "        # save critic checkpoint\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': critic.state_dict(),\n",
    "                    'optimizer_state_dict': opt_critic.state_dict(),\n",
    "                    'loss': loss_critic,\n",
    "        }, f\"{DISC_PATH}epoch-{epoch}.tar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
