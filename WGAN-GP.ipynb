{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, math\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skfmm\n",
    "\n",
    "import GAN as GAN\n",
    "from GAN import Generator\n",
    "from GAN import Critic\n",
    "\n",
    "import wandb\n",
    "\n",
    "import data_manager as dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_METRICS = True\n",
    "\n",
    "# Inputs\n",
    "DATASET = 'random_40_density'\n",
    "SUBSET = 'train'\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "# Structure\n",
    "NUM_LAYERS_CRIT = 6\n",
    "KERNEL_CRIT = [\"all 4\"]\n",
    "STRIDE_CRIT = [2,2,2,2,1,1]\n",
    "PAD_CRIT = [\"all 'same' (as seen in Keras)\"]\n",
    "FEATURES_CRIT = [3,64,128,256,512,512,1]\n",
    "\n",
    "NUM_LAYERS_GEN = 16\n",
    "KERNEL_GEN = [\"all 4\"]\n",
    "STRIDE_GEN = [\"all 2\"]\n",
    "PAD_GEN = [\"all 'same' (as seen in Keras)\"]\n",
    "FEATURES_GEN = [2,64,128,256,512,512,512,512,512,512,512,512,512,256,128,64,1]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LR_CRIT = 2e-4\n",
    "LR_GEN = 2e-4\n",
    "CRIT_ITERATIONS = 5\n",
    "LAMBDA = 10\n",
    "\n",
    "\n",
    "# Internal Data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAP_SHAPE = (64,64)\n",
    "# NOISE_SHAPE = (BATCH_SIZE, 1, MAP_SHAPE[0], MAP_SHAPE[1])\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP=''\n",
    "\n",
    "CONFIG = dict(\n",
    "    dataset = DATASET,\n",
    "    subset = SUBSET,\n",
    "\n",
    "    layers_crit = NUM_LAYERS_CRIT,\n",
    "    kernels_crit = KERNEL_CRIT,\n",
    "    stride_crit = STRIDE_CRIT,\n",
    "    padding_crit = PAD_CRIT,\n",
    "    features_crit = FEATURES_CRIT,\n",
    "\n",
    "    layers_gen = NUM_LAYERS_GEN,\n",
    "    kernels_gen = KERNEL_GEN,\n",
    "    stride_gen = STRIDE_GEN,\n",
    "    padding_gen = PAD_GEN,\n",
    "    features_gen = FEATURES_GEN,\n",
    "\n",
    "    batch_size = BATCH_SIZE,\n",
    "    learning_rate_crit = LR_CRIT,\n",
    "    learning_rate_gen = LR_GEN,\n",
    "    crit_iterations = CRIT_ITERATIONS,\n",
    "    gp_coefficient = LAMBDA\n",
    ")\n",
    "\n",
    "if RECORD_METRICS:\n",
    "    run = wandb.init(project='wgan-gp', entity='aicv-lab', config=CONFIG, group=GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The GAN's Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GAN's definitions and hyperparams\n",
    "if RECORD_METRICS:\n",
    "    dm.save_gan(run.name, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Essential Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(coeff, critic, real, fake, device=\"cpu\"):\n",
    "    # sample x_hat from P(x_hat)\n",
    "    rand = torch.randn((real.shape[0], 1, 1, 1), device=device) # generate a random number from 0 to 1 for each matrix in the batch\n",
    "    x_hat = rand*real + (1-rand)*fake\n",
    "\n",
    "    critic_output = critic(x_hat)\n",
    "    grad_ones = torch.ones_like(critic_output, device=device)\n",
    "\n",
    "    gp = torch.autograd.grad(                                   # find magnitude of critic's resulting gradient\n",
    "        inputs = x_hat,\n",
    "        outputs = critic_output,\n",
    "        grad_outputs = grad_ones,\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "\n",
    "    gp = torch.norm(gp, p=2, dim=(1,2,3))    # vector norm of each gradient\n",
    "    gp = (gp - 1)**2\n",
    "    gp = coeff * torch.mean(gp)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, subset, device='cpu'):\n",
    "        self.device = device\n",
    "        self.paths = dm.load_input(dataset, subset) # Load all of the paths in the specified set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.paths[idx]\n",
    "        x = x.to(self.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PathsDataset(DATASET, SUBSET, device=device)\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch = START_EPOCH\n",
    "\n",
    "gen = Generator(FEATURES_GEN, KERNEL_GEN, STRIDE_GEN, PAD_GEN, device=device)\n",
    "critic = Critic(FEATURES_CRIT, KERNEL_CRIT, STRIDE_CRIT, PAD_CRIT, device=device)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LR_GEN, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LR_CRIT, betas = (0.0, 0.9))\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_noise = torch.rand(NOISE_SHAPE, device=device)  # Uncomment when giving gen input with random noise\n",
    "\n",
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    curr_epoch += 1\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = F.interpolate(real, size=(256,256))  # pix2pix requires 256x256 inputs\n",
    "\n",
    "        initial_path = real[:,1:2,:,:]\n",
    "        # fixed_input = torch.concat((fixed_noise, real[:,1:,:,:]), axis=1)  # Uncomment when giving gen input with random noise\n",
    "        fixed_input = real[:,1:,:,:]  # Uncomment when giving gen input without random noise\n",
    "\n",
    "        for _ in range(CRIT_ITERATIONS):\n",
    "            # noise = torch.randn_like(real[:,-1:,:,:], device=device).abs()  # Uncomment when giving gen input with random noise\n",
    "            # noise = torch.concat((noise, real[:,1:,:,:]), axis=1)  # Uncomment when giving gen input with random noise\n",
    "            noise = real[:,1:,:,:]  # Uncomment when giving gen input without random noise\n",
    "\n",
    "            fake = gen(noise)\n",
    "            fake = torch.concat((fake, real[:,1:,:,:]), axis=1)\n",
    "\n",
    "            critic_real = critic(real)\n",
    "            critic_fake = critic(fake)\n",
    "            gp = gradient_penalty(LAMBDA, critic, real, fake, device=device) # compute the gradient penalty\n",
    "            loss_critic = (\n",
    "                torch.mean(critic_fake) - torch.mean(critic_real) + gp\n",
    "            )\n",
    "\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{curr_epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \" +\n",
    "                  f\"Loss D: {loss_critic:.4f}, Lambda GP: {gp:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            if RECORD_METRICS:\n",
    "                dm.save_checkpoint(run.name, run.step, gen, critic)\n",
    "\n",
    "            if BATCH_SIZE > 8:\n",
    "                outputs = gen(fixed_input[:8,:,:,:])\n",
    "                inputs = real[:8,:,:,:]\n",
    "                # outputs = torch.concat((outputs, fixed_input[:8,1:,:,:]), axis=1)  # Uncomment when giving gen input with random noise\n",
    "                outputs = torch.concat((outputs, fixed_input[:8,:,:,:]), axis=1)  # Uncomment when giving gen input without random noise\n",
    "            else:\n",
    "                outputs = gen(fixed_input)\n",
    "                inputs = real\n",
    "                # outputs = torch.concat((outputs, fixed_input[:,1:,:,:]), axis=1)  # Uncomment when giving gen input with random noise\n",
    "                outputs = torch.concat((outputs, fixed_input), axis=1)  # Uncomment when giving gen input without random noise\n",
    "\n",
    "            if RECORD_METRICS:\n",
    "                wandb.log({\n",
    "                    'epoch': curr_epoch,\n",
    "                    'generator loss': loss_gen,\n",
    "                    'critic loss': loss_critic,\n",
    "                    'gradient penalty': gp,\n",
    "                    'fake': wandb.Image(outputs),\n",
    "                    'real': wandb.Image(inputs)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECORD_METRICS:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
