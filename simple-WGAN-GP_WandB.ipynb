{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, math\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skfmm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_METRICS = True\n",
    "\n",
    "# Inputs\n",
    "MAP_NAME = 'map_64x64'\n",
    "DATASET = 'training'\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "# Structure\n",
    "NUM_LAYERS_CRIT = 5\n",
    "KERNEL_CRIT = [4,4,4,4,4]\n",
    "STRIDE_CRIT = [2,2,2,2,1]\n",
    "PAD_CRIT = [1,1,1,1,0]\n",
    "FEATURES_CRIT = [3,64,128,256,512]\n",
    "\n",
    "NUM_LAYERS_GEN = 10\n",
    "KERNEL_GEN = [4,4,4,4,4,4,4,4,4,4]\n",
    "STRIDE_GEN = [2,2,2,2,1,1,2,2,2,2]\n",
    "PAD_GEN = [1,1,1,1,0,0,1,1,1,1]\n",
    "FEATURES_GEN = [3,64,128,256,512,1024,512,256,128,64]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LR_CRIT = 1e-5\n",
    "LR_GEN = 1e-5\n",
    "CRIT_ITERATIONS = 5\n",
    "LAMBDA = 10\n",
    "\n",
    "\n",
    "# Internal Data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAP_SHAPE = (64,64)\n",
    "NOISE_SHAPE = (BATCH_SIZE, 1, MAP_SHAPE[0], MAP_SHAPE[1])\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP=''\n",
    "\n",
    "CONFIG = dict(\n",
    "    map_name = MAP_NAME,\n",
    "    dataset = DATASET,\n",
    "\n",
    "    layers_crit = NUM_LAYERS_CRIT,\n",
    "    kernels_crit = KERNEL_CRIT,\n",
    "    stride_crit = STRIDE_CRIT,\n",
    "    padding_crit = PAD_CRIT,\n",
    "    features_crit = FEATURES_CRIT,\n",
    "\n",
    "    layers_gen = NUM_LAYERS_GEN,\n",
    "    kernels_gen = KERNEL_GEN,\n",
    "    stride_gen = STRIDE_GEN,\n",
    "    padding_gen = PAD_GEN,\n",
    "    features_gen = FEATURES_GEN,\n",
    "\n",
    "    batch_size = BATCH_SIZE,\n",
    "    learning_rate_crit = LR_CRIT,\n",
    "    learning_rate_gen = LR_GEN,\n",
    "    crit_iterations = CRIT_ITERATIONS,\n",
    "    gp_coefficient = LAMBDA\n",
    ")\n",
    "\n",
    "if RECORD_METRICS:\n",
    "    run = wandb.init(project='wgan-gp', entity='aicv-lab', config=CONFIG, group=GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The GAN's Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, f, k, s, p, device='cpu'):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(f[0], f[1], k[0], s[0], p[0], device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f[1], f[2], k[1], s[1], p[1], device=device),\n",
    "            nn.InstanceNorm2d(f[2], affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(f[2], f[3], k[2], s[2], p[2], device=device),\n",
    "            nn.InstanceNorm2d(f[3], affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(f[3], f[4], k[3], s[3], p[3], device=device),\n",
    "            nn.InstanceNorm2d(f[4], affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(f[4], 1, k[4], s[4], p[4], device=device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "        y = self.block5(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input channels (noise, map, initial path)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, f, k, s, p, device='cpu'):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(f[0], f[1], k[0], s[0], p[0], device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f[1], f[2], k[1], s[1], p[1], device=device),\n",
    "            nn.InstanceNorm2d(f[2], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn. Conv2d(f[2], f[3], k[2], s[2], p[2], device=device),\n",
    "            nn.InstanceNorm2d(f[3], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn. Conv2d(f[3], f[4], k[3], s[3], p[3], device=device),\n",
    "            nn.InstanceNorm2d(f[4], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn. Conv2d(f[4], f[5], k[4], s[4], p[4], device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(f[5], f[6], k[5], s[5], p[5], device=device),\n",
    "            nn.InstanceNorm2d(f[6], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(f[6], f[7], k[6], s[6], p[6], device=device),\n",
    "            nn.InstanceNorm2d(f[7], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(f[7], f[8], k[7], s[7], p[7], device=device),\n",
    "            nn.InstanceNorm2d(f[8], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block9 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(f[8], f[9], k[8], s[8], p[8], device=device),\n",
    "            nn.InstanceNorm2d(f[9], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.block10 = nn.Sequential(\n",
    "            nn. ConvTranspose2d(f[9], 1, k[9], s[9], p[9], device=device),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "        y = self.block5(y)\n",
    "        y = self.block6(y)\n",
    "        y = self.block7(y)\n",
    "        y = self.block8(y)\n",
    "        y = self.block9(y)\n",
    "        y = self.block10(y)\n",
    "\n",
    "        y = self._round(y)\n",
    "        return y\n",
    "    \n",
    "    def _round(self, mat):\n",
    "        # TODO: cite something? (this function is based off of Thor's code)\n",
    "        mat_hard = torch.round(mat)\n",
    "        mat = (mat_hard - mat.data) + mat\n",
    "\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model definitions\n",
    "\n",
    "# Save the generator definition\n",
    "savepath = os.path.join(os.getcwd(), 'checkpoints', run.name)\n",
    "if not os.path.isdir(savepath):\n",
    "    os.makedirs(savepath)\n",
    "shutil.copy(f'./GAN.py', os.path.join(savepath, 'GAN.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Essential Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(coeff, critic, real, fake, device=\"cpu\"):\n",
    "    # sample x_hat from P(x_hat)\n",
    "    rand = torch.randn((real.shape[0], 1, 1, 1), device=device) # generate a random number from 0 to 1 for each matrix in the batch\n",
    "    x_hat = rand*real + (1-rand)*fake\n",
    "\n",
    "    critic_output = critic(x_hat)\n",
    "    grad_ones = torch.ones_like(critic_output, device=device)\n",
    "\n",
    "    gp = torch.autograd.grad(                                   # find magnitude of critic's resulting gradient\n",
    "        inputs = x_hat,\n",
    "        outputs = critic_output,\n",
    "        grad_outputs = grad_ones,\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "\n",
    "    gp = torch.norm(gp, p=2, dim=(1,2,3))    # vector norm of each gradient\n",
    "    gp = (gp - 1)**2\n",
    "    gp = coeff * torch.mean(gp)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path_dir, map_file, transform=None, shape = (100,100), device='cpu'):\n",
    "        self.device = device\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        self.map = np.loadtxt(map_file, skiprows=2).reshape(shape)\n",
    "        self.map = self.map[np.newaxis, :, :]\n",
    "        for filename in os.listdir(path_dir):\n",
    "            with open(os.path.join(path_dir, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Add Initial Path onto the loaded path matrix\n",
    "        initial_path = np.zeros_like(path_mat)\n",
    "\n",
    "        # Create Straight line between start/end points\n",
    "        x1 = path[0,0]\n",
    "        y1 = path[0,1]\n",
    "        x2 = path[path.shape[0]-1,0]\n",
    "        y2 = path[path.shape[0]-1,1]\n",
    "\n",
    "        x = x1\n",
    "        y = y1\n",
    "\n",
    "        if (x1 < x2):\n",
    "            x_dir = 1\n",
    "        else:\n",
    "            x_dir = -1\n",
    "\n",
    "        if (y1 < y2):\n",
    "            y_dir = 1\n",
    "        else:\n",
    "            y_dir = -1\n",
    "\n",
    "        # Determine y from x\n",
    "        if x2-x1 != 0:\n",
    "            m = (y2-y1)/(x2-x1)\n",
    "            while x != x2:\n",
    "                y = round(m*(x-x1) + y1)\n",
    "                initial_path[y,x] = 1\n",
    "                x += x_dir\n",
    "        else:\n",
    "            while x != x2:\n",
    "                initial_path[y1,x] = 1\n",
    "                x += x_dir\n",
    "\n",
    "        x = x1\n",
    "        y = y1\n",
    "        # Determine x from y\n",
    "        if y2-y1 != 0:\n",
    "            m = (x2-x1)/(y2-y1)\n",
    "            while y != y2:\n",
    "                x = round(m*(y-y1) + x1)\n",
    "                initial_path[y,x] = 1\n",
    "                y += y_dir\n",
    "        else:\n",
    "            while y != y2:\n",
    "                initial_path[y,x1] = 1\n",
    "                y += y_dir\n",
    "\n",
    "        initial_path[y1,x1] = 1     # Include the first point in the path\n",
    "        initial_path[y2,x2] = 1     # Include the last point in the path\n",
    "\n",
    "        path_mat = np.stack((path_mat, initial_path))\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_from_map(map, device=device, max=1, min=-1, slope=0.1):\n",
    "    map[map == 0] = min # remap the map to be between -1 and 1\n",
    "    map[map == 1] = max\n",
    "    sd = skfmm.distance(map, dx = slope) # compute signed distance\n",
    "    sd = torch.Tensor(-sd).to(device)# turn sd into a tensor\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "map = np.loadtxt(f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", skiprows=2).reshape(MAP_SHAPE)\n",
    "map = map[np.newaxis,np.newaxis,:,:]\n",
    "map = np.repeat(map, BATCH_SIZE, axis=0)\n",
    "map = grad_from_map(map)\n",
    "\n",
    "train_dataset = PathsDataset(path_dir = f\"./env/{MAP_NAME}/paths/{DATASET}\", map_file = f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=None, device=device)\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch = START_EPOCH\n",
    "\n",
    "gen = Generator(FEATURES_GEN, KERNEL_GEN, STRIDE_GEN, PAD_GEN, device=device)\n",
    "critic = Critic(FEATURES_CRIT, KERNEL_CRIT, STRIDE_CRIT, PAD_CRIT,device=device)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LR_GEN, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LR_CRIT, betas = (0.0, 0.9))\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(NOISE_SHAPE, device=device).abs()\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# writer_overlay = SummaryWriter(f\"logs/fake_overlay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Batch 0/468 Loss D: 32.0964, Lambda GP: 337.1546, loss G: -1.5879\n",
      "Epoch [1/500] Batch 100/468 Loss D: -12.1294, Lambda GP: 34.8987, loss G: 27.8032\n",
      "Epoch [1/500] Batch 200/468 Loss D: -7.7926, Lambda GP: 21.1115, loss G: 24.2308\n",
      "Epoch [1/500] Batch 300/468 Loss D: -8.2358, Lambda GP: 18.5229, loss G: 26.4175\n",
      "Epoch [1/500] Batch 400/468 Loss D: -8.5411, Lambda GP: 13.8237, loss G: 27.1672\n",
      "Epoch [2/500] Batch 0/468 Loss D: -9.3875, Lambda GP: 21.2846, loss G: 27.4417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9958/1589228987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcritic_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mcritic_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAMBDA_GP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute the gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             loss_critic = (\n\u001b[1;32m     23\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9958/3084477683.py\u001b[0m in \u001b[0;36mgradient_penalty\u001b[0;34m(coeff, critic, real, fake, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrad_ones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     gp = torch.autograd.grad(                                   # find magnitude of critic's resulting gradient\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    curr_epoch += 1\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        real = torch.concat((real, map), axis=1)\n",
    "\n",
    "        initial_path = real[:,1:2,:,:]\n",
    "        fixed_input = torch.concat((fixed_noise, initial_path, map), axis=1)\n",
    "\n",
    "\n",
    "        for _ in range(CRIT_ITERATIONS):\n",
    "            noise = torch.randn(NOISE_SHAPE, device=device).abs()\n",
    "            noise = torch.concat((noise, initial_path, map), axis=1)\n",
    "\n",
    "            fake = gen(noise)\n",
    "            fake = torch.concat((fake, initial_path, map), axis=1)\n",
    "            critic_real = critic(real)\n",
    "            critic_fake = critic(fake)\n",
    "            gp = gradient_penalty(LAMBDA, critic, real, fake, device=device) # compute the gradient penalty\n",
    "            # gp = 0\n",
    "            loss_critic = (\n",
    "                torch.mean(critic_fake) - torch.mean(critic_real) + gp\n",
    "            )\n",
    "\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{curr_epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \" +\n",
    "                  f\"Loss D: {loss_critic:.4f}, Lambda GP: {gp:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            if RECORD_METRICS:\n",
    "                savepath = os.path.join(os.getcwd(), 'checkpoints', run.name, 'gen')\n",
    "                if not os.path.isdir(savepath):\n",
    "                    os.makedirs(savepath)\n",
    "                torch.save({\n",
    "                            'dataset': DATASET,\n",
    "                            'config': CONFIG,\n",
    "                            'state': gen.state_dict()\n",
    "                            },\n",
    "                            os.path.join(savepath, f'step_{run.step}.tar'))\n",
    "\n",
    "                # save critic checkpoint\n",
    "                savepath = os.path.join(os.getcwd(), 'checkpoints', run.name, 'crit')\n",
    "                if not os.path.isdir(savepath):\n",
    "                    os.makedirs(savepath)\n",
    "                torch.save({\n",
    "                            'dataset': DATASET,\n",
    "                            'config': CONFIG,\n",
    "                            'state': critic.state_dict()\n",
    "                            },\n",
    "                            os.path.join(savepath, f'step_{run.step}.tar'))\n",
    "\n",
    "            if BATCH_SIZE > 8:\n",
    "        torch.save(gen, f\"{GEN_PATH}epoch-{epoch}.pt\")\n",
    "                outputs = gen(fixed_input[:8,:,:,:])\n",
    "                inputs = real[:8,:,:,:]\n",
    "                outputs = torch.concat((outputs, fixed_input[:8,1:2,:,:], map[:8,:,:,:]), axis=1)\n",
    "            else:\n",
    "                outputs = gen(fixed_input)\n",
    "                inputs = real\n",
    "                outputs = torch.concat((outputs, fixed_input[:,1:2,:,:], map), axis=1)\n",
    "\n",
    "            if RECORD_METRICS:\n",
    "                wandb.log({\n",
    "                    'epoch': curr_epoch,\n",
    "                    'generator loss': loss_gen,\n",
    "                    'critic loss': loss_critic,\n",
    "                    'gradient penalty': gp,\n",
    "                    'fake': wandb.Image(outputs),\n",
    "                    'real': wandb.Image(inputs)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECORD_METRICS:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('GAN-NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccf3d60b9b1cdec8f7c51fe623365f849c25434c0843b09b573fa1626b7830e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
