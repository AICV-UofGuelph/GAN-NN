{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/GANs/2.%20DCGAN/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "MAP_NAME = 'map_64x64'\n",
    "DATASET = 'smooth_paths'\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "# Structure\n",
    "NUM_LAYERS_CRIT = 4\n",
    "KERNEL_CRIT = [3,3,3,3]\n",
    "PAD_CRIT = [1,1,1,1]\n",
    "FEATURES_CRIT = [64,128,64]\n",
    "\n",
    "NUM_LAYERS_GEN = 4\n",
    "KERNEL_GEN = [3,3,3,3]\n",
    "PAD_GEN = [1,1,1,1]\n",
    "FEATURES_GEN = [64,128,64]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LR_CRIT = 1e-5\n",
    "LR_GEN = 1e-5\n",
    "CRIT_ITERATIONS = 5\n",
    "LAMBDA = 10\n",
    "\n",
    "\n",
    "# Internal Data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAP_SHAPE = (64,64)\n",
    "NOISE_SHAPE = (BATCH_SIZE, 1, MAP_SHAPE[0], MAP_SHAPE[1])\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP=''\n",
    "\n",
    "CONFIG = dict(\n",
    "    map_name = MAP_NAME,\n",
    "    dataset = DATASET,\n",
    "\n",
    "    num_layers_critic = NUM_LAYERS_CRIT,\n",
    "    kernel_sizes_critic = KERNEL_CRIT,\n",
    "    padding_critic = PAD_CRIT,\n",
    "    num_features_critic = FEATURES_CRIT,\n",
    "\n",
    "    num_layers_gen = NUM_LAYERS_GEN,\n",
    "    kernel_size_gen = KERNEL_GEN,\n",
    "    padding_gen = PAD_GEN,\n",
    "    num_features_gen = FEATURES_GEN,\n",
    "\n",
    "    batch_size = BATCH_SIZE,\n",
    "    learning_rate_critic = LR_CRIT,\n",
    "    learning_rate_gen = LR_GEN,\n",
    "    critic_iterations = CRIT_ITERATIONS,\n",
    "    gp_coefficient = LAMBDA\n",
    ")\n",
    "\n",
    "wandb.init(project='wgan-gp', entity='aicv-lab', config=CONFIG, group=GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The GAN's Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, FEATURES_CRIT[0], KERNEL_CRIT[0], 1, PAD_CRIT[0], device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(FEATURES_CRIT[0], FEATURES_CRIT[1], KERNEL_CRIT[1], 1, PAD_CRIT[1], bias=False, device=device),\n",
    "            nn.InstanceNorm2d(FEATURES_CRIT[1], affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(FEATURES_CRIT[1], FEATURES_CRIT[2], KERNEL_CRIT[2], 1, PAD_CRIT[2], bias=False, device=device),\n",
    "            nn.InstanceNorm2d(FEATURES_CRIT[2], affine=True, device=device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(FEATURES_CRIT[2], 1, KERNEL_CRIT[3], 1, PAD_CRIT[3], device=device), # convert to single channel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "\n",
    "        y = torch.mean(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input channels (noise, map, initial path)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, FEATURES_GEN[0], KERNEL_GEN[0], 1, PAD_GEN[0], device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(FEATURES_GEN[0], FEATURES_GEN[1], KERNEL_GEN[1], 1, PAD_GEN[1], device=device),\n",
    "            nn.InstanceNorm2d(FEATURES_GEN[1], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn. Conv2d(FEATURES_GEN[1], FEATURES_GEN[2], KERNEL_GEN[2], 1, PAD_GEN[2], device=device),\n",
    "            nn.InstanceNorm2d(FEATURES_GEN[2], affine=True, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn. Conv2d(FEATURES_GEN[2], 1, KERNEL_GEN[3], 1, PAD_GEN[3], device=device),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = self.block3(y)\n",
    "        y = self.block4(y)\n",
    "\n",
    "        # y = F.adaptive_max_pool2d(y, output_size=map_shape)\n",
    "\n",
    "        y = self._round(y)\n",
    "        return y\n",
    "    \n",
    "    def _round(self, mat):\n",
    "        # TODO: cite something? (this function is based off of Thor's code)\n",
    "        mat_hard = torch.round(mat)\n",
    "        mat = (mat_hard - mat.data) + mat\n",
    "\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Essential Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(coeff, critic, real, fake, device=\"cpu\"):\n",
    "    # sample x_hat from P(x_hat)\n",
    "    rand = torch.randn((real.shape[0], 1, 1, 1), device=device) # generate a random number from 0 to 1 for each matrix in the batch\n",
    "    x_hat = rand*real + (1-rand)*fake\n",
    "\n",
    "    critic_output = critic(x_hat)\n",
    "    grad_ones = torch.ones_like(critic_output, device=device)\n",
    "\n",
    "    gp = torch.autograd.grad(                                   # find magnitude of critic's resulting gradient\n",
    "        inputs = x_hat,\n",
    "        outputs = critic_output,\n",
    "        grad_outputs = grad_ones,\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "\n",
    "    gp = torch.norm(gp, p=2, dim=(1,2,3))    # vector norm of each gradient\n",
    "    gp = (gp - 1)**2\n",
    "    gp = coeff * torch.mean(gp)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to override __init__, __len__, __getitem__\n",
    "# as per datasets requirement\n",
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path_dir, map_file, transform=None, shape = (100,100), device='cpu'):\n",
    "        self.device = device\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        self.map = np.loadtxt(map_file, skiprows=2).reshape(shape)\n",
    "        self.map = self.map[np.newaxis, :, :]\n",
    "        for filename in os.listdir(path_dir):\n",
    "            with open(os.path.join(path_dir, filename), 'r') as f: # open in readonly mode\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=int).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "\n",
    "                self.path_matrix = self.convert_path(shape, self.path)\n",
    "                \n",
    "                self.paths.append(self.path_matrix) # add the path to paths list\n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "            \n",
    "        path_mat[path[path.shape[0]-1,1], path[path.shape[0]-1,0]] = 1     # Include the last point in the path\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Add Initial Path onto the loaded path matrix\n",
    "        initial_path = np.zeros_like(path_mat)\n",
    "\n",
    "        # Create Straight line between start/end points\n",
    "        x1 = path[0,0]\n",
    "        y1 = path[0,1]\n",
    "        x2 = path[path.shape[0]-1,0]\n",
    "        y2 = path[path.shape[0]-1,1]\n",
    "\n",
    "        x = x1\n",
    "        y = y1\n",
    "\n",
    "        if (x1 < x2):\n",
    "            x_dir = 1\n",
    "        else:\n",
    "            x_dir = -1\n",
    "\n",
    "        if (y1 < y2):\n",
    "            y_dir = 1\n",
    "        else:\n",
    "            y_dir = -1\n",
    "\n",
    "        # Determine y from x\n",
    "        if x2-x1 != 0:\n",
    "            m = (y2-y1)/(x2-x1)\n",
    "            while x != x2:\n",
    "                y = round(m*(x-x1) + y1)\n",
    "                initial_path[y,x] = 1\n",
    "                x += x_dir\n",
    "        else:\n",
    "            while x != x2:\n",
    "                initial_path[y1,x] = 1\n",
    "                x += x_dir\n",
    "\n",
    "        x = x1\n",
    "        y = y1\n",
    "        # Determine x from y\n",
    "        if y2-y1 != 0:\n",
    "            m = (x2-x1)/(y2-y1)\n",
    "            while y != y2:\n",
    "                x = round(m*(y-y1) + x1)\n",
    "                initial_path[y,x] = 1\n",
    "                y += y_dir\n",
    "        else:\n",
    "            while y != y2:\n",
    "                initial_path[y,x1] = 1\n",
    "                y += y_dir\n",
    "\n",
    "        initial_path[y1,x1] = 1     # Include the first point in the path\n",
    "        initial_path[y2,x2] = 1     # Include the last point in the path\n",
    "\n",
    "        # slope = -0.05\n",
    "\n",
    "        # for x in range(0, len(initial_path)):\n",
    "        #     for y in range(0, len(initial_path[x])):\n",
    "        #         dis_start = math.sqrt((x-x1)**2 + (y-y1)**2)\n",
    "        #         dis_goal = math.sqrt((x-x2)**2 + (y-y2)**2)\n",
    "        #         dis = dis_start if dis_start < dis_goal else dis_goal\n",
    "\n",
    "        #         height = slope*dis + 1\n",
    "\n",
    "        #         if height < 0:\n",
    "        #             initial_path[y][x] = 0\n",
    "        #         else:\n",
    "        #             initial_path[y][x] = height\n",
    "\n",
    "        path_mat = np.stack((path_mat, initial_path))\n",
    "\n",
    "        return path_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.float32(self.paths[idx])\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        #return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = np.loadtxt(f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", skiprows=2).reshape(MAP_SHAPE)\n",
    "map = map[np.newaxis,np.newaxis,:,:]\n",
    "map = np.repeat(map, BATCH_SIZE, axis=0)\n",
    "map = torch.Tensor(map).to(device)\n",
    "\n",
    "train_dataset = PathsDataset(path_dir = f\"./env/{MAP_NAME}/paths/{DATASET}/training\", map_file = f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=None, device=device)\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch = START_EPOCH\n",
    "\n",
    "gen = Generator(device=device)\n",
    "critic = Critic(device=device)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LR_GEN, betas = (0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LR_CRIT, betas = (0.0, 0.9))\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(NOISE_SHAPE, device=device).abs()\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# writer_overlay = SummaryWriter(f\"logs/fake_overlay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    curr_epoch += 1\n",
    "    for batch_idx, real in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        real = torch.concat((real, map), axis=1)\n",
    "\n",
    "        initial_path = real[:,1:2,:,:]\n",
    "        fixed_input = torch.concat((fixed_noise, initial_path, map), axis=1)\n",
    "\n",
    "        for _ in range(CRIT_ITERATIONS):\n",
    "            noise = torch.randn(NOISE_SHAPE, device=device).abs()\n",
    "            noise = torch.concat((noise, initial_path, map), axis=1)\n",
    "            fake = gen(noise)\n",
    "            fake = torch.concat((fake, initial_path, map), axis=1)\n",
    "            critic_real = critic(real)\n",
    "            critic_fake = critic(fake)\n",
    "            gp = gradient_penalty(LAMBDA, critic, real, fake, device=device) # compute the gradient penalty\n",
    "            loss_critic = (\n",
    "                torch.mean(critic_fake) - torch.mean(critic_real) + gp\n",
    "            )\n",
    "\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        ### Training generator: min E(critic(gen_fake))\n",
    "        output = critic(fake)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{curr_epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \" +     # TODO: print correct ending epoch based on initial (loaded) epoch num\n",
    "                  f\"Loss D: {loss_critic:.4f}, Lambda GP: {gp:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            # TODO: Generate example outputs\n",
    "            if BATCH_SIZE > 8:\n",
    "                outputs = gen(fixed_input[:8,:,:,:])\n",
    "                inputs = real[:8,:,:,:]\n",
    "                outputs = torch.concat((outputs, fixed_input[:8,1:2,:,:], map[:8,:,:,:]), axis=1)\n",
    "            else:\n",
    "                outputs = gen(fixed_input)\n",
    "                inputs = real\n",
    "                outputs = torch.concat((outputs, fixed_input[:,1:2,:,:], map), axis=1)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': curr_epoch,\n",
    "                'generator loss': loss_gen,\n",
    "                'critic loss': loss_critic,\n",
    "                'gradient penalty': gp,\n",
    "                'fake': wandb.Image(outputs),\n",
    "                'real': wandb.Image(inputs)\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
